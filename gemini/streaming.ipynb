{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62d019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "  langchain-core  \\\n",
    "  langchain[google-genai]  \\\n",
    "  langchain-community  \\\n",
    "  langsmith  \\\n",
    "  google-search-results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767be970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.0) \n",
    "\n",
    "\n",
    "#from langchain.chat_models import init_chat_model\n",
    "\n",
    "#llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b619832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "\n",
    "async for token in llm.astream(\"What is NLP\"):\n",
    "    tokens.append(token)\n",
    "    print(token.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dae7315",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeb677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(x: float, y: float) -> float:\n",
    "    \"\"\"Add 'x' and 'y'.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "# Define the multiply tool\n",
    "@tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiply 'x' and 'y'.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "# Define the exponentiate tool\n",
    "@tool\n",
    "def exponentiate(x: float, y: float) -> float:\n",
    "    \"\"\"Raise 'x' to the power of 'y'.\"\"\"\n",
    "    return x ** y\n",
    "\n",
    "@tool\n",
    "def subtract(x: float, y: float) -> float:\n",
    "    \"\"\"Subtract 'x' from 'y'.\"\"\"\n",
    "    return y - x\n",
    "\n",
    "@tool\n",
    "def final_answer(answer: str, tools_used:list[str]) -> str:\n",
    "    \"\"\"Use this tool to provide a final answer to the user.\n",
    "    The answer should be in natural language as this will be provided\n",
    "    to the user directly. The tools_used must include a list of tool\n",
    "    names that were used within the `scratchpad`.\n",
    "    \"\"\"\n",
    "    return {\"answer\": answer, \"tools_used\": tools_used}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba34d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt= ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You're a helpful assistant. When answering a user's question \"\n",
    "        \"you should first use one of the tools provided. After using a \"\n",
    "        \"tool the tool output will be provided back to you. You MUST \"\n",
    "        \"then use the final_answer tool to provide a final answer to the user. \"\n",
    "        \"DO NOT use the same tool more than once.\"\n",
    "    )),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bff779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.base import RunnableSerializable\n",
    "\n",
    "tools = [add, subtract, multiply, exponentiate, final_answer]\n",
    "\n",
    "# define the agent runnable\n",
    "agent: RunnableSerializable = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.bind_tools(tools, tool_choice=\"any\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddbca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "# create tool name to function mapping\n",
    "name2tool = {tool.name: tool.func for tool in tools}\n",
    "\n",
    "class CustomAgentExecutor:\n",
    "    chat_history: list[BaseMessage]\n",
    "\n",
    "    def __init__(self, max_iterations: int = 3):\n",
    "        self.chat_history = []\n",
    "        self.max_iterations = max_iterations\n",
    "        self.agent: RunnableSerializable = (\n",
    "            {\n",
    "                \"input\": lambda x: x[\"input\"],\n",
    "                \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "                \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "            }\n",
    "            | prompt\n",
    "            | llm.bind_tools(tools, tool_choice=\"any\")  # we're forcing tool use again\n",
    "        )\n",
    "\n",
    "    def invoke(self, input: str) -> dict:\n",
    "        # invoke the agent but we do this iteratively in a loop until\n",
    "        # reaching a final answer\n",
    "        count = 0\n",
    "        agent_scratchpad = []\n",
    "        while count < self.max_iterations:\n",
    "            # invoke a step for the agent to generate a tool call\n",
    "            out = self.agent.invoke({\n",
    "                \"input\": input,\n",
    "                \"chat_history\": self.chat_history,\n",
    "                \"agent_scratchpad\": agent_scratchpad\n",
    "            })\n",
    "            # if the tool call is the final answer tool, we stop\n",
    "            if out.tool_calls[0][\"name\"] == \"final_answer\":\n",
    "                break\n",
    "            agent_scratchpad.append(out)  # add tool call to scratchpad\n",
    "            # otherwise we execute the tool and add it's output to the agent scratchpad\n",
    "            tool_out = name2tool[out.tool_calls[0][\"name\"]](**out.tool_calls[0][\"args\"])\n",
    "            # add the tool output to the agent scratchpad\n",
    "            action_str = f\"The {out.tool_calls[0]['name']} tool returned {tool_out}\"\n",
    "            agent_scratchpad.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": action_str,\n",
    "                \"tool_call_id\": out.tool_calls[0][\"id\"]\n",
    "            })\n",
    "            # add a print so we can see intermediate steps\n",
    "            print(f\"{count}: {action_str}\")\n",
    "            count += 1\n",
    "        # add the final output to the chat history\n",
    "        final_answer = out.tool_calls[0][\"args\"]\n",
    "        # this is a dictionary, so we convert it to a string for compatibility with\n",
    "        # the chat history\n",
    "        final_answer_str = json.dumps(final_answer)\n",
    "        self.chat_history.append({\"input\": input, \"output\": final_answer_str})\n",
    "        self.chat_history.extend([\n",
    "            HumanMessage(content=input),\n",
    "            AIMessage(content=final_answer_str)\n",
    "        ])\n",
    "        # return the final answer in dict form\n",
    "        return final_answer\n",
    "\n",
    "agent_executor = CustomAgentExecutor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1422d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke(input=\"What is 10 + 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa2c045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\", \n",
    "    temperature=0.0\n",
    ").configurable_fields(\n",
    "    callbacks = ConfigurableField(\n",
    "        id=\"callbacks\",\n",
    "        name=\"callbacks\",\n",
    "        description=\"A list of callbacks to use for streaming\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2f84fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent: RunnableSerializable = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.bind_tools(tools, tool_choice=\"any\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15c7585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from langchain.callbacks.base import AsyncCallbackHandler\n",
    "\n",
    "\n",
    "class QueueCallbackHandler(AsyncCallbackHandler):\n",
    "    \"\"\"Callback handler that puts tokens into a queue.\"\"\"\n",
    "\n",
    "    def __init__(self, queue: asyncio.Queue):\n",
    "        self.queue = queue\n",
    "        self.final_answer_seen = False\n",
    "\n",
    "    async def __aiter__(self):\n",
    "        while True:\n",
    "            if self.queue.empty():\n",
    "                await asyncio.sleep(0.1)\n",
    "                continue\n",
    "            token_or_done = await self.queue.get()\n",
    "\n",
    "            if token_or_done == \"<<DONE>>\":\n",
    "                # this means we're done\n",
    "                return\n",
    "            if token_or_done:\n",
    "                yield token_or_done\n",
    "\n",
    "    async def on_llm_new_token(self, *args, **kwargs) -> None:\n",
    "        \"\"\"Put new token in the queue.\"\"\"\n",
    "        #print(f\"on_llm_new_token: {args}, {kwargs}\")\n",
    "        chunk = kwargs.get(\"chunk\")\n",
    "        if chunk:\n",
    "            # check for final_answer tool call\n",
    "            if tool_calls := chunk.message.additional_kwargs.get(\"tool_calls\"):\n",
    "                if tool_calls[0][\"function\"][\"name\"] == \"final_answer\":\n",
    "                    # this will allow the stream to end on the next `on_llm_end` call\n",
    "                    self.final_answer_seen = True\n",
    "        self.queue.put_nowait(kwargs.get(\"chunk\"))\n",
    "        return\n",
    "\n",
    "    async def on_llm_end(self, *args, **kwargs) -> None:\n",
    "        \"\"\"Put None in the queue to signal completion.\"\"\"\n",
    "        #print(f\"on_llm_end: {args}, {kwargs}\")\n",
    "        # this should only be used at the end of our agent execution, however LangChain\n",
    "        # will call this at the end of every tool call, not just the final tool call\n",
    "        # so we must only send the \"done\" signal if we have already seen the final_answer\n",
    "        # tool call\n",
    "        if self.final_answer_seen:\n",
    "            self.queue.put_nowait(\"<<DONE>>\")\n",
    "        else:\n",
    "            self.queue.put_nowait(\"<<STEP_END>>\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab9b075",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = asyncio.Queue()\n",
    "\n",
    "streamer = QueueCallbackHandler(queue)\n",
    "\n",
    "tokens = []\n",
    "\n",
    "async def stream(query: str):\n",
    "    response = agent.with_config(\n",
    "        callbacks = [streamer]\n",
    "    )\n",
    "    \n",
    "    async for token in response.astream({\n",
    "        \"input\": query,\n",
    "        \"chat_history\": [],\n",
    "        \"agent_scratchpad\": []\n",
    "    }): \n",
    "        tokens.append(token)\n",
    "        print(token, flush = True)\n",
    "    \n",
    "\n",
    "await stream(\"what is 10 + 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5968c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "class CustomAgentExecutor:\n",
    "    chat_history: list[BaseMessage]\n",
    "\n",
    "    def __init__(self, max_iterations: int = 3):\n",
    "        self.chat_history = []\n",
    "        self.max_iterations = max_iterations\n",
    "        self.agent: RunnableSerializable = (\n",
    "            {\n",
    "                \"input\": lambda x: x[\"input\"],\n",
    "                \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "                \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "            }\n",
    "            | prompt\n",
    "            | llm.bind_tools(tools, tool_choice=\"any\")  # we're forcing tool use again\n",
    "        )\n",
    "\n",
    "    async def invoke(self, input: str, streamer: QueueCallbackHandler, verbose: bool = False) -> dict:\n",
    "        # invoke the agent but we do this iteratively in a loop until\n",
    "        # reaching a final answer\n",
    "        count = 0\n",
    "        agent_scratchpad = []\n",
    "        while count < self.max_iterations:\n",
    "            # invoke a step for the agent to generate a tool call\n",
    "            async def stream(query: str):\n",
    "                response = self.agent.with_config(\n",
    "                    callbacks=[streamer]\n",
    "                )\n",
    "                # we initialize the output dictionary that we will be populating with\n",
    "                # our streamed output\n",
    "                output = None\n",
    "                # now we begin streaming\n",
    "                async for token in response.astream({\n",
    "                    \"input\": query,\n",
    "                    \"chat_history\": self.chat_history,\n",
    "                    \"agent_scratchpad\": agent_scratchpad\n",
    "                }):\n",
    "                    if output is None:\n",
    "                        output = token\n",
    "                    else:\n",
    "                        # we can just add the tokens together as they are streamed and\n",
    "                        # we'll have the full response object at the end\n",
    "                        output += token\n",
    "                    if token.content != \"\":\n",
    "                        # we can capture various parts of the response object\n",
    "                        if verbose: print(f\"content: {token.content}\", flush=True)\n",
    "                    tool_calls = token.additional_kwargs.get(\"tool_calls\")\n",
    "                    if tool_calls:\n",
    "                        if verbose: print(f\"tool_calls: {tool_calls}\", flush=True)\n",
    "                        tool_name = tool_calls[0][\"function\"][\"name\"]\n",
    "                        if tool_name:\n",
    "                            if verbose: print(f\"tool_name: {tool_name}\", flush=True)\n",
    "                        arg = tool_calls[0][\"function\"][\"arguments\"]\n",
    "                        if arg != \"\":\n",
    "                            if verbose: print(f\"arg: {arg}\", flush=True)\n",
    "                return AIMessage(\n",
    "                    content=output.content,\n",
    "                    tool_calls=output.tool_calls,\n",
    "                    tool_call_id=output.tool_calls[0][\"id\"]\n",
    "                )\n",
    "\n",
    "            tool_call = await stream(query=input)\n",
    "            # add initial tool call to scratchpad\n",
    "            agent_scratchpad.append(tool_call)\n",
    "            # otherwise we execute the tool and add it's output to the agent scratchpad\n",
    "            tool_name = tool_call.tool_calls[0][\"name\"]\n",
    "            tool_args = tool_call.tool_calls[0][\"args\"]\n",
    "            tool_call_id = tool_call.tool_call_id\n",
    "            tool_out = name2tool[tool_name](**tool_args)\n",
    "            # add the tool output to the agent scratchpad\n",
    "            tool_exec = ToolMessage(\n",
    "                content=f\"{tool_out}\",\n",
    "                tool_call_id=tool_call_id\n",
    "            )\n",
    "            agent_scratchpad.append(tool_exec)\n",
    "            count += 1\n",
    "            # if the tool call is the final answer tool, we stop\n",
    "            if tool_name == \"final_answer\":\n",
    "                break\n",
    "        # add the final output to the chat history, we only add the \"answer\" field\n",
    "        final_answer = tool_out[\"answer\"]\n",
    "        self.chat_history.extend([\n",
    "            HumanMessage(content=input),\n",
    "            AIMessage(content=final_answer)\n",
    "        ])\n",
    "        # return the final answer in dict form\n",
    "        return tool_args\n",
    "\n",
    "agent_executor = CustomAgentExecutor(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aa05b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = asyncio.Queue()\n",
    "streamer = QueueCallbackHandler(queue)\n",
    "\n",
    "await agent_executor.invoke(\"What is nine plus 10, minus 4 * 2 to the power of 3\", streamer, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68c4467",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = asyncio.Queue()\n",
    "streamer = QueueCallbackHandler(queue)\n",
    "\n",
    "out = await agent_executor.invoke(\"What is 10 + 10\", streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e9c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = asyncio.Queue()\n",
    "streamer = QueueCallbackHandler(queue)\n",
    "\n",
    "task = asyncio.create_task(agent_executor.invoke(\"What is 10 + 10\", streamer))\n",
    "\n",
    "async for token in streamer:\n",
    "    print(token, flush=True)\n",
    "\n",
    "await task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa0965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = asyncio.Queue()\n",
    "streamer = QueueCallbackHandler(queue)\n",
    "\n",
    "task = asyncio.create_task(agent_executor.invoke(\"What is 10 + 10\", streamer))\n",
    "\n",
    "async for token in streamer:\n",
    "    # first identify if we have a <<STEP_END>> token\n",
    "    if token == \"<<STEP_END>>\":\n",
    "        print(\"\\n\", flush=True)\n",
    "    # we'll first identify if the token is a tool call\n",
    "    elif tool_calls := token.message.additional_kwargs.get(\"tool_calls\"):\n",
    "        # if we have a tool call with a tool name, we'll print it\n",
    "        if tool_name := tool_calls[0][\"function\"][\"name\"]:\n",
    "            print(f\"Calling {tool_name}...\", flush=True)\n",
    "        # if we have a tool call with arguments, we ad them to our args string\n",
    "        if tool_args := tool_calls[0][\"function\"][\"arguments\"]:\n",
    "            print(f\"{tool_args}\", end=\"\", flush=True)\n",
    "\n",
    "_ = await task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb69d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
