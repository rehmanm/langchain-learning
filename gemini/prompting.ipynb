{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df725308",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "  langchain-core \\\n",
    "  langchain-google-genai \\\n",
    "  langchain-community \\\n",
    "  scikit-image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74b4d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c59d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "\n",
    "Answer the user's query based on the context below.\n",
    "If you don't know the answer based on the context\n",
    "provided, please respond, I don't know.\n",
    "\n",
    "---\n",
    "Context: {context}\n",
    "---\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd2a538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", prompt),\n",
    "        (\"user\", \"{query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef411e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb13bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3201996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate\n",
    ")\n",
    "\n",
    "prompt_template_2 = ChatPromptTemplate.from_messages([\n",
    "        SystemMessagePromptTemplate.from_template(prompt),\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4e1f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_2.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baeb966",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_2.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2271103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\", temperature=0.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc5e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = (\n",
    "    {\n",
    "        \"query\": lambda x: x[\"query\"],\n",
    "        \"context\": lambda x: x[\"context\"]\n",
    "    }\n",
    "    | prompt_template_2 \n",
    "    | llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4bcd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Aurelio AI is an AI company developing tooling for AI\n",
    "engineers. There focus is on the language AI with the team having strong\n",
    "expertise in building AI agents and a strong background in information\n",
    "retreival.\n",
    "\n",
    "The company is behind several open source frameworks, most notably\n",
    "Semantic Router and Semantic Chunkers. They also have an AI \n",
    "Platform providing engineers with tooling to help them build with \n",
    "AI. Finally, the team also provides development services to other\n",
    "organizations to help them bring their AI tech to market.\n",
    "\n",
    "Aurelio AI become langchain expert in September 2024 after a long \n",
    "track record of delivering AI solutions built with Langchain ecosystem.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "query =\"What does Aurelio AI do?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb91f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.invoke({\"query\": query, \"context\": context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8376b389",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.invoke({\"query\": \"What does Microsoft do\", \"context\": context})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368ce884",
   "metadata": {},
   "source": [
    "## Few Short Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1931373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = ChatPromptTemplate.from_messages([ \n",
    "        HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "        AIMessagePromptTemplate.from_template(\"{output}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a4714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"input\": \"Here is the query #1\", \"output\": \"Here is the answer #1\"},\n",
    "    {\"input\": \"Here is the query #2\", \"output\": \"Here is the answer #2\"},\n",
    "    {\"input\": \"Here is the query #3\", \"output\": \"Here is the answer #3\"}    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8069be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt = example_prompt,\n",
    "    examples = examples\n",
    ")\n",
    "\n",
    "print(few_shot_prompt.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908e6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_system_prompt = \"\"\"\n",
    "\n",
    "Answer the user's query based on the context below.\n",
    "If you don't know the answer based on the context\n",
    "provided, please respond, I don't know.\n",
    "\n",
    "Always answser in markdown format. When doing so please\n",
    "provide headers, short summaries, follow with bullet \n",
    "points, then conclude.\n",
    "\n",
    "---\n",
    "Context: {context}\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_markdown = ChatPromptTemplate.from_messages([\n",
    "        SystemMessagePromptTemplate.from_template(new_system_prompt),\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\")\n",
    "])\n",
    " \n",
    "\n",
    "pipeline_markdown = (\n",
    "    {\n",
    "        \"query\": lambda x: x[\"query\"],\n",
    "        \"context\": lambda x: x[\"context\"]\n",
    "    }\n",
    "    | prompt_template_markdown \n",
    "    | llm)\n",
    "\n",
    "out = pipeline_markdown.invoke({\"query\": query, \"context\": context}).content\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a96a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69da0329",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Can you explain gravity?\",\n",
    "        \"output\": (\n",
    "            \"## Gravity\\n\\n\"\n",
    "            \"Gravity is one of the fundamental forces in the universe.\\n\\n\"\n",
    "            \"### Discovery\\n\\n\"\n",
    "            \"* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\\n\"\n",
    "            \"* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\\n\\n\"\n",
    "            \"### In General Relativity\\n\\n\"\n",
    "            \"* Gravity is described as the curvature of spacetime.\\n\"\n",
    "            \"* The more massive an object is, the more it curves spacetime.\\n\"\n",
    "            \"* This curvature is what causes objects to fall towards each other.\\n\\n\"\n",
    "            \"### Gravitons\\n\\n\"\n",
    "            \"* Gravitons are hypothetical particles that mediate the force of gravity.\\n\"\n",
    "            \"* They have not yet been detected.\\n\\n\"\n",
    "            \"**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\\n\\n\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is the capital of France?\",\n",
    "        \"output\": (\n",
    "            \"## France\\n\\n\"\n",
    "            \"The capital of France is Paris.\\n\\n\"\n",
    "            \"### Origins\\n\\n\"\n",
    "            \"* The name Paris comes from the Latin word \\\"Parisini\\\" which referred to a Celtic people living in the area.\\n\"\n",
    "            \"* The Romans named the city Lutetia, which means \\\"the place where the river turns\\\".\\n\"\n",
    "            \"* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\\n\\n\"\n",
    "            \"**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world's greatest cultural and economic centres.\\n\\n\"\n",
    "        )\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155e9a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt_new = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt = example_prompt,\n",
    "    examples = examples\n",
    ")\n",
    "\n",
    "out = few_shot_prompt_new.format()\n",
    "\n",
    "display(Markdown(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a54d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt_template = ChatPromptTemplate.from_messages([\n",
    "        SystemMessagePromptTemplate.from_template(new_system_prompt),\n",
    "        FewShotChatMessagePromptTemplate(\n",
    "            example_prompt = example_prompt,\n",
    "            examples = examples\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70547484",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f696dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_few_shot = (\n",
    "    {\n",
    "        \"query\": lambda x: x[\"query\"],\n",
    "        \"context\": lambda x: x[\"context\"]\n",
    "    }\n",
    "    | few_shot_prompt_template \n",
    "    | llm)\n",
    "\n",
    "out = pipeline_few_shot.invoke({\"query\": query, \"context\": context}).content\n",
    "\n",
    "display(Markdown(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394ae8e0",
   "metadata": {},
   "source": [
    "## Chain of Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76acb759",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cot_system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\n",
    "You MUST answer the question directly without any other\n",
    "text or explanation.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "no_cot_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(no_cot_system_prompt),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf6367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many keystrokes are needed to type the numbers from 1 to 500?\"\n",
    "\n",
    "no_cot_pipeline = (\n",
    "    { \"query\": lambda x: x[\"query\"]}\n",
    "    | no_cot_prompt_template\n",
    "    | llm\n",
    ")\n",
    "\n",
    "no_cot_result = no_cot_pipeline.invoke({\"query\": query}).content\n",
    "print(no_cot_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfae2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\n",
    "To answer the question you must:\n",
    "\n",
    "- List systematically and precisely detail all\n",
    "  subproblems that needs to be solved to answer the\n",
    "  question.\n",
    "- Solve each subproblem INDIVIDUALLY and in sequence.\n",
    "- Finally, use everything you have worked through to \n",
    "  provide the final answer. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cot_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(cot_system_prompt),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f3fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_pipeline = (\n",
    "    { \"query\": lambda x: x[\"query\"]}\n",
    "    | cot_prompt_template\n",
    "    | llm\n",
    ")\n",
    "\n",
    "cot_result = cot_pipeline.invoke({\"query\": query}).content\n",
    "display(Markdown(cot_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ad303",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\"\"\"\n",
    "\n",
    "no_cot_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(default_system_prompt),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b851cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_pipeline = (\n",
    "    { \"query\": lambda x: x[\"query\"]}\n",
    "    | cot_prompt_template\n",
    "    | llm\n",
    ")\n",
    "\n",
    "default_result = default_pipeline.invoke({\"query\": query}).content\n",
    "display(Markdown(default_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22899aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
