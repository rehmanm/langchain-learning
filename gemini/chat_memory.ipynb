{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1738225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "  langchain-core \\\n",
    "  langchain-google-genai \\\n",
    "  langchain-community \\\n",
    "  scikit-image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f8a346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f10416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "\n",
    "llm = GoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c23e2ac",
   "metadata": {},
   "source": [
    "## Conversation Buffer Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b210e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d5e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(\n",
    "    {\"input\": \"Hi, my names in James\"},\n",
    "    {\"output\": \"Hey James, what's up? I'm an AI model called Zeta.\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I am researching different types of conversational memory.\"},\n",
    "    {\"output\": \"That's interesting, what are some examples?\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I am researching ConversationBufferMemory and ConversationBufferWindowMemory\"},\n",
    "    {\"output\": \"That's interesting, what's thed difference\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer memory just store the enitre converation, right?\"},\n",
    "    {\"output\": \"Thats makes sense, what about ConversationBufferWindowMemory\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer window memory stores last k messages, dropping the rest.\"},\n",
    "    {\"output\": \"Very cool!\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c404e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f173409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7fe9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": \"What is my name again?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c4b963",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory with RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb6b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant called Zeta.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\")   \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e56847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = (\n",
    "    { \n",
    "        \"query\": lambda x: x[\"query\"],\n",
    "        \"history\": lambda x: x[\"history\"]\n",
    "    }\n",
    "    | prompt_template\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f01f42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chat_map = {}\n",
    "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19e8257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09185e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b09506",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc00a359",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8f1a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=4, return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(\n",
    "    {\"input\": \"Hi, my names in James\"},\n",
    "    {\"output\": \"Hey James, what's up? I'm an AI model called Zeta.\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I am researching different types of conversational memory.\"},\n",
    "    {\"output\": \"That's interesting, what are some examples?\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I am researching ConversationBufferMemory and ConversationBufferWindowMemory\"},\n",
    "    {\"output\": \"That's interesting, what's thed difference\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer memory just store the enitre converation, right?\"},\n",
    "    {\"output\": \"Thats makes sense, what about ConversationBufferWindowMemory\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer window memory stores last k messages, dropping the rest.\"},\n",
    "    {\"output\": \"Very cool!\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276af513",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ed9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd5e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": \"What is my name again\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6809097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    k: int = Field(default_factory=int)\n",
    "    \n",
    "    \n",
    "    def __init__(self, k:int):\n",
    "        super().__init__(k=k)\n",
    "        print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
    "        \n",
    "    \n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"\n",
    "        Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages\n",
    "        \"\"\"\n",
    "        \n",
    "        self.messages.extend(messages)\n",
    "        self.messages = self.messages[-self.k:]\n",
    "        \n",
    "        \n",
    "    def clear(self) -> None:\n",
    "        \"\"\"\n",
    "        clear the history\n",
    "        \"\"\"\n",
    "        self.messages = []       \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0653e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "def get_chat_window_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
    "    \n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115022d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "pipeline_cwbm = (\n",
    "    { \n",
    "        \"query\": lambda x: x[\"query\"],\n",
    "        \"history\": lambda x: x[\"history\"]\n",
    "    }\n",
    "    | prompt_template\n",
    "    | llm\n",
    ")\n",
    " \n",
    "\n",
    "pipeline_with_history_cwbm = RunnableWithMessageHistory(\n",
    "    pipeline_cwbm,\n",
    "    get_session_history = get_chat_window_history,\n",
    "    input_messages_key = \"query\",\n",
    "    history_messages_key = \"history\",\n",
    "    history_factory_config = [\n",
    "        ConfigurableFieldSpec(\n",
    "            id = \"session_id\",\n",
    "            annotation = str,\n",
    "            name = \"Session ID\",\n",
    "            description = \"The session ID to use for the chat history\",\n",
    "            default = \"id_default\"\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id = \"k\",\n",
    "            annotation = int,\n",
    "            name = \"k\",\n",
    "            description = \"The number of messages to keep in the history\",\n",
    "            default = 4\n",
    "        )\n",
    "    ]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de016f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history_cwbm.invoke(\n",
    "    {\n",
    "        \"query\": \"my name is James\"  \n",
    "    },\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"session_id\" : \"id_k3\",\n",
    "            \"k\" : 3\n",
    "        }\n",
    "    }\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6a9e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history_cwbm.invoke(\n",
    "    {\n",
    "        \"query\": \"I am learning langchain\"  \n",
    "    },\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"session_id\" : \"id_k3\",\n",
    "            \"k\" : 3\n",
    "        }\n",
    "    }\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c51b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history_cwbm.invoke(\n",
    "    {\n",
    "        \"query\": \"My plan is to learn Langgraph and Google A2A, which one should I go\"  \n",
    "    },\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"session_id\" : \"id_k3\",\n",
    "            \"k\" : 3\n",
    "        }\n",
    "    }\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6a4030",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map[\"id_k3\"].messages[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff34ba35",
   "metadata": {},
   "source": [
    "## Converasation Summary Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68053a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "summary_memory = ConversationSummaryMemory(llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4facf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = summary_memory,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb45ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chain.invoke({\"input\": \"hello there my name in Muhammad\"})\n",
    "summary_chain.invoke({\"input\": \"I am learning chain and exploring how it can be use\"})\n",
    "summary_chain.invoke({\"input\": \"I am looking into ConversationBufferMemory and ConversationWindowBufferMemory\"})\n",
    "summary_chain.invoke({\"input\": \"BufferMemory stores the entire conversation\"})\n",
    "summary_chain.invoke({\"input\": \"BufferWindowMemory stores last k messages and dropped previous messages\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d888de88",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chain.invoke({\"input\": \"What is my name\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439167eb",
   "metadata": {},
   "source": [
    "## Conversation Summary Memory with Runnable Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8617c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0afec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel): \n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: GoogleGenerativeAI = Field(default_factory=GoogleGenerativeAI)\n",
    "    \n",
    "    def __init__(self, llm:GoogleGenerativeAI):\n",
    "        super().__init__(llm=llm)\n",
    "    \n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history\n",
    "        \"\"\"\n",
    "        self.messages.extend(messages)\n",
    "        # construct the summary chat messages\n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"Given the existing conversation summary and the new messages, \"\n",
    "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                \"as much relevant information as possible.\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
    "                \"New messages:\\n{messages}\"\n",
    "            )\n",
    "        ])\n",
    "        # format the messages and invoke the LLM\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=self.messages,\n",
    "                messages=[x.content for x in messages]\n",
    "            )\n",
    "        )\n",
    "        # replace the existing history with a single system summary message\n",
    "        self.messages = [SystemMessage(content=new_summary)]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f38841",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chat_map = {}\n",
    "\n",
    "def get_summary_chat_history(session_id: str, llm: GoogleGenerativeAI) -> ConversationSummaryMessageHistory:\n",
    "    if session_id not in summary_chat_map:\n",
    "        summary_chat_map[session_id] = ConversationSummaryMessageHistory(llm = llm)\n",
    "    \n",
    "    return summary_chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74cbbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_csmh = (\n",
    "        { \n",
    "        \"query\": lambda x: x[\"query\"],\n",
    "        \"history\": lambda x: x[\"history\"]\n",
    "        }\n",
    "    | prompt_template\n",
    "    | llm\n",
    ")\n",
    "\n",
    "pipeline_with_summary_history = RunnableWithMessageHistory(\n",
    "    pipeline_csmh,\n",
    "    get_session_history = get_summary_chat_history,\n",
    "    input_messages_key = \"query\",\n",
    "    history_messages_key = \"history\",\n",
    "    history_factory_config = [\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=GoogleGenerativeAI,\n",
    "            name=\"LLM\",\n",
    "            description=\"The LLM to use for the conversation summary\",\n",
    "            default=llm,\n",
    "        )\n",
    "        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18961c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = \"csbm_123\"\n",
    "\n",
    "pipeline_with_summary_history.invoke(\n",
    "    {\n",
    "        \"query\": \"Hi, my name is Muhammad\"\n",
    "    },\n",
    "    config={\n",
    "        \"session_id\": session_id, \n",
    "        \"llm\": llm\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ea49a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chat_map[session_id].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1aee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_summary_history.invoke(\n",
    "    {\n",
    "        \"query\": \"I'm researching different type of conversational memory\"\n",
    "    },\n",
    "    config={\n",
    "        \"session_id\": session_id, \n",
    "        \"llm\": llm\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ecdadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in [\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]:\n",
    "    pipeline_with_summary_history.invoke(\n",
    "        {\"query\": msg},\n",
    "        config={\"session_id\": session_id, \"llm\": llm}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b355035",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chat_map[session_id].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c604959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    pipeline_with_summary_history.invoke(\n",
    "        {\"query\": \"what is my name again\"},\n",
    "        config={\"session_id\": session_id, \"llm\": llm}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bc892a",
   "metadata": {},
   "source": [
    "### ConversationSummaryBufferMemory\n",
    "\n",
    "Our final memory type acts as a combination of ConversationSummaryMemory and ConversationBufferMemory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cc7588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=300,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "csbm1_chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e64b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "csbm1_chain.invoke({\"input\": \"Hi, my name is James\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722014d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in [\n",
    "    \"I'm researching the different types of conversational memory.\",\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]:\n",
    "    csbm1_chain.invoke({\"input\": msg})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d779899",
   "metadata": {},
   "source": [
    "### ConversationSummaryBufferMemory with RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ac045",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummaryBufferMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: GoogleGenerativeAI = Field(default_factory=GoogleGenerativeAI)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, llm: GoogleGenerativeAI, k: int):\n",
    "        super().__init__(llm=llm, k=k)\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages and summarizing the messages that we\n",
    "        drop.\n",
    "        \"\"\"\n",
    "        existing_summary: SystemMessage | None = None\n",
    "        old_messages: list[BaseMessage] | None = None\n",
    "        # see if we already have a summary message\n",
    "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
    "            print(\">> Found existing summary\")\n",
    "            existing_summary = self.messages.pop(0)\n",
    "        # add the new messages to the history\n",
    "        self.messages.extend(messages)\n",
    "        # check if we have too many messages\n",
    "        if len(self.messages) > self.k:\n",
    "            print(\n",
    "                f\">> Found {len(self.messages)} messages, dropping \"\n",
    "                f\"oldest {len(self.messages) - self.k} messages.\"\n",
    "            )\n",
    "            # pull out the oldest messages...\n",
    "            old_messages = self.messages[:self.k]\n",
    "            # ...and keep only the most recent messages\n",
    "            self.messages = self.messages[-self.k:]\n",
    "        if old_messages is None:\n",
    "            print(\">> No old messages to update summary with\")\n",
    "            # if we have no old_messages, we have nothing to update in summary\n",
    "            return\n",
    "        # construct the summary chat messages\n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"Given the existing conversation summary and the new messages, \"\n",
    "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                \"as much relevant information as possible.\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
    "                \"New messages:\\n{old_messages}\"\n",
    "            )\n",
    "        ])\n",
    "        # format the messages and invoke the LLM\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=existing_summary,\n",
    "                old_messages=old_messages\n",
    "            )\n",
    "        )\n",
    "        print(f\">> New summary: {new_summary}\")\n",
    "        # prepend the new summary to the history\n",
    "        self.messages = [SystemMessage(content=new_summary)] + self.messages\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed24aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map_csbm_1 = {}\n",
    "def get_chat_history_csbm_1(session_id: str, llm: GoogleGenerativeAI, k: int) -> ConversationSummaryBufferMessageHistory:\n",
    "    \n",
    "    if session_id not in chat_map_csbm_1:\n",
    "        print(f\"session not exists {session_id}\")\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map_csbm_1[session_id] = ConversationSummaryBufferMessageHistory(llm=llm, k=k)\n",
    "    # return the chat history\n",
    "    return chat_map_csbm_1[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b05e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_csbmh = (\n",
    "        { \n",
    "        \"query\": lambda x: x[\"query\"],\n",
    "        \"history\": lambda x: x[\"history\"]\n",
    "        }\n",
    "    | prompt_template\n",
    "    | llm\n",
    ")\n",
    "\n",
    "\n",
    "pipeline_with_history_csbm_1 = RunnableWithMessageHistory(\n",
    "    pipeline_csbmh,\n",
    "    get_session_history=get_chat_history_csbm_1,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=GoogleGenerativeAI,\n",
    "            name=\"LLM\",\n",
    "            description=\"The LLM to use for the conversation summary\",\n",
    "            default=llm,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\",\n",
    "            annotation=int,\n",
    "            name=\"k\",\n",
    "            description=\"The number of messages to keep in the history\",\n",
    "            default=4,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee2832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "csbm_session_id = \"csbm_123\"\n",
    "pipeline_with_history_csbm_1.invoke(\n",
    "        {\"query\": \"Hi, my name is Muhammad\"},\n",
    "        config={\"session_id\": csbm_session_id, \"llm\": llm, \"k\": 4}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf1def",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, msg in enumerate([\n",
    "    \"I'm researching the different types of conversational memory.\",\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]):\n",
    "    print(f\"---\\nMessage {i+1}\\n---\\n\")\n",
    "    pipeline_with_history_csbm_1.invoke(\n",
    "        {\"query\": msg},\n",
    "        config={\"session_id\": csbm_session_id, \"llm\": llm, \"k\": 4}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bb7257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
