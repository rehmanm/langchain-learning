{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1738225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "  langchain-core \\\n",
    "  langchain-google-genai \\\n",
    "  langchain-community \\\n",
    "  scikit-image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8f8a346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter API key for Google Gemini: ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28f10416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\", temperature=0.0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c23e2ac",
   "metadata": {},
   "source": [
    "## Conversation Buffer Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b210e0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1363/1448044083.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0324a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(\n",
    "    {\"input\": \"Hi, my names in James\"},\n",
    "    {\"output\": \"Hey James, what's up? I'm an AI model called Zeta.\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I am researching different types of conversational memory.\"},\n",
    "    {\"output\": \"That's interesting, what are some examples?\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I am researching ConversationBufferMemory and ConversationBufferWindowMemory\"},\n",
    "    {\"output\": \"That's interesting, what's thed difference\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer memory just store the enitre converation, right?\"},\n",
    "    {\"output\": \"Thats makes sense, what about ConversationBufferWindowMemory\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer window memory stores last k messages, dropping the rest.\"},\n",
    "    {\"output\": \"Very cool!\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4de9d659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, my names in James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='I am researching different types of conversational memory.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='I am researching ConversationBufferMemory and ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's thed difference\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just store the enitre converation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Thats makes sense, what about ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2775dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1363/2943370833.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9095ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my names in James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='I am researching different types of conversational memory.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='I am researching ConversationBufferMemory and ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's thed difference\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just store the enitre converation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='Thats makes sense, what about ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
      "Human: What is my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name again?',\n",
       " 'history': [HumanMessage(content='Hi, my names in James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='I am researching different types of conversational memory.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='I am researching ConversationBufferMemory and ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's thed difference\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just store the enitre converation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Thats makes sense, what about ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is my name again?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Your name is James.', additional_kwargs={}, response_metadata={})],\n",
       " 'response': 'Your name is James.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is my name again?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009e6e8d",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory with RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f28b3ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant called Zeta.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\")   \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1399533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = (\n",
    "    { \"query\": lambda x: x[\"query\"],\n",
    "    \"history\": lambda x: x[\"history\"]}\n",
    "    | prompt_template\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e5c9a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chat_map = {}\n",
    "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c2f90ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eed34873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi James, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--adcbe9ea-bde0-480c-9a7b-1d5fd32b6d21-0', usage_metadata={'input_tokens': 14, 'output_tokens': 19, 'total_tokens': 33, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6e14d4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is James.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--ca735b9c-0f9a-48f8-a11f-99112cbba5f6-0', usage_metadata={'input_tokens': 38, 'output_tokens': 6, 'total_tokens': 44, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc7272b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
