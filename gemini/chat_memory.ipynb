{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1738225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "  langchain-core \\\n",
    "  langchain-google-genai \\\n",
    "  langchain-community \\\n",
    "  scikit-image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8f8a346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter API key for Google Gemini: ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28f10416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\", temperature=0.0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c23e2ac",
   "metadata": {},
   "source": [
    "## Conversation Buffer Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b210e0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9782/1448044083.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51d5e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(\n",
    "    {\"input\": \"Hi, my names in James\"},\n",
    "    {\"output\": \"Hey James, what's up? I'm an AI model called Zeta.\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I am researching different types of conversational memory.\"},\n",
    "    {\"output\": \"That's interesting, what are some examples?\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I am researching ConversationBufferMemory and ConversationBufferWindowMemory\"},\n",
    "    {\"output\": \"That's interesting, what's thed difference\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer memory just store the enitre converation, right?\"},\n",
    "    {\"output\": \"Thats makes sense, what about ConversationBufferWindowMemory\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer window memory stores last k messages, dropping the rest.\"},\n",
    "    {\"output\": \"Very cool!\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5c404e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, my names in James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='I am researching different types of conversational memory.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='I am researching ConversationBufferMemory and ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's thed difference\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just store the enitre converation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Thats makes sense, what about ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f173409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9782/2943370833.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d7fe9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my names in James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='I am researching different types of conversational memory.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='I am researching ConversationBufferMemory and ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's thed difference\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just store the enitre converation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='Thats makes sense, what about ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
      "Human: What is my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name again?',\n",
       " 'history': [HumanMessage(content='Hi, my names in James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='I am researching different types of conversational memory.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='I am researching ConversationBufferMemory and ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's thed difference\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just store the enitre converation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Thats makes sense, what about ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is my name again?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Your name is James.', additional_kwargs={}, response_metadata={})],\n",
       " 'response': 'Your name is James.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is my name again?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c4b963",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory with RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beb6b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant called Zeta.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\")   \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e56847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = (\n",
    "    { \n",
    "        \"query\": lambda x: x[\"query\"],\n",
    "        \"history\": lambda x: x[\"history\"]\n",
    "    }\n",
    "    | prompt_template\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f01f42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chat_map = {}\n",
    "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d19e8257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09185e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi James, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--af6717c4-0140-48c4-952d-32905b94331b-0', usage_metadata={'input_tokens': 14, 'output_tokens': 19, 'total_tokens': 33, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00b09506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is James.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--fbfcf1ee-5ceb-48e7-96af-9fa1060589df-0', usage_metadata={'input_tokens': 38, 'output_tokens': 6, 'total_tokens': 44, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc00a359",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de8f1a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9782/3216785012.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=4, return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=4, return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c550165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(\n",
    "    {\"input\": \"Hi, my names in James\"},\n",
    "    {\"output\": \"Hey James, what's up? I'm an AI model called Zeta.\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I am researching different types of conversational memory.\"},\n",
    "    {\"output\": \"That's interesting, what are some examples?\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I am researching ConversationBufferMemory and ConversationBufferWindowMemory\"},\n",
    "    {\"output\": \"That's interesting, what's thed difference\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer memory just store the enitre converation, right?\"},\n",
    "    {\"output\": \"Thats makes sense, what about ConversationBufferWindowMemory\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer window memory stores last k messages, dropping the rest.\"},\n",
    "    {\"output\": \"Very cool!\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5279ed94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='I am researching different types of conversational memory.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='I am researching ConversationBufferMemory and ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's thed difference\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just store the enitre converation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Thats makes sense, what about ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03399ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f05e5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='I am researching different types of conversational memory.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='I am researching ConversationBufferMemory and ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's thed difference\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just store the enitre converation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='Thats makes sense, what about ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
      "Human: What is my name again\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name again',\n",
       " 'history': [HumanMessage(content='I am researching different types of conversational memory.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='I am researching ConversationBufferMemory and ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's thed difference\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just store the enitre converation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Thats makes sense, what about ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})],\n",
       " 'response': 'I am sorry, I do not have access to that information. I do not have access to your name.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is my name again\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a4cd10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    k: int = Field(default_factory=int)\n",
    "    \n",
    "    \n",
    "    def __init__(self, k:int):\n",
    "        super().__init__(k=k)\n",
    "        print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
    "        \n",
    "    \n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"\n",
    "        Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages\n",
    "        \"\"\"\n",
    "        \n",
    "        self.messages.extend(messages)\n",
    "        self.messages = self.messages[-self.k:]\n",
    "        \n",
    "        \n",
    "    def clear(self) -> None:\n",
    "        \"\"\"\n",
    "        clear the history\n",
    "        \"\"\"\n",
    "        self.messages = []       \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b78c3f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "def get_chat_window_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
    "    \n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36f85ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "pipeline_cwbm = (\n",
    "    { \n",
    "        \"query\": lambda x: x[\"query\"],\n",
    "        \"history\": lambda x: x[\"history\"]\n",
    "    }\n",
    "    | prompt_template\n",
    "    | llm\n",
    ")\n",
    " \n",
    "\n",
    "pipeline_with_history_cwbm = RunnableWithMessageHistory(\n",
    "    pipeline_cwbm,\n",
    "    get_session_history = get_chat_window_history,\n",
    "    input_messages_key = \"query\",\n",
    "    history_messages_key = \"history\",\n",
    "    history_factory_config = [\n",
    "        ConfigurableFieldSpec(\n",
    "            id = \"session_id\",\n",
    "            annotation = str,\n",
    "            name = \"Session ID\",\n",
    "            description = \"The session ID to use for the chat history\",\n",
    "            default = \"id_default\"\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id = \"k\",\n",
    "            annotation = int,\n",
    "            name = \"k\",\n",
    "            description = \"The number of messages to keep in the history\",\n",
    "            default = 4\n",
    "        )\n",
    "    ]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1de1b2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You already told me that, James! Is there anything I can help you with? Perhaps you'd like to tell me something else about yourself, or ask me a question?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--aa8c285b-23d8-4d1c-b33e-3173baeaeb65-0', usage_metadata={'input_tokens': 30, 'output_tokens': 37, 'total_tokens': 67, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history_cwbm.invoke(\n",
    "    {\n",
    "        \"query\": \"my name is James\"  \n",
    "    },\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"session_id\" : \"id_k3\",\n",
    "            \"k\" : 3\n",
    "        }\n",
    "    }\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68baac6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"That's great, James! Langchain is a powerful tool. What are you hoping to achieve with Langchain? Knowing your goals will help me provide more relevant information and assistance.\\n\\nFor example, are you interested in:\\n\\n*   **Building a specific type of application?** (e.g., a chatbot, a document summarizer, a question-answering system)\\n*   **Learning about a particular aspect of Langchain?** (e.g., chains, agents, memory, models)\\n*   **Troubleshooting a problem you're encountering?**\\n*   **Finding resources for learning Langchain?**\\n\\nTell me a bit more about what you're working on, and I'll do my best to help!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--b4f1f898-81bf-415f-bc9b-1e926c4ce5d2-0', usage_metadata={'input_tokens': 67, 'output_tokens': 155, 'total_tokens': 222, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history_cwbm.invoke(\n",
    "    {\n",
    "        \"query\": \"I am learning langchain\"  \n",
    "    },\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"session_id\" : \"id_k3\",\n",
    "            \"k\" : 3\n",
    "        }\n",
    "    }\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83b88972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Okay, that's a great plan! Both LangGraph and Google's A2A (presumably you mean Vertex AI's Agent Builder, which is sometimes referred to as A2A) are powerful tools for building AI applications, but they serve different purposes and have different learning curves. Here's a breakdown to help you decide which to tackle first:\\n\\n**LangGraph:**\\n\\n*   **Focus:** Orchestration and state management for complex, multi-actor LLM applications. Think of it as a framework for building robust and reliable workflows involving multiple LLM calls, conditional logic, and state transitions.\\n*   **Use Cases:** Ideal for applications that require complex decision-making, iterative refinement, and handling of different scenarios. Examples include:\\n    *   Complex chatbots with multiple tools and conversational flows.\\n    *   Automated agents that perform tasks involving multiple steps and dependencies.\\n    *   Applications that require reasoning and planning.\\n*   **Learning Curve:** Steeper initially. You'll need to understand the concepts of graphs, nodes, edges, and state management. However, once you grasp the fundamentals, it provides a very powerful and flexible way to structure your applications.\\n*   **Prerequisites:** A good understanding of Langchain fundamentals (chains, agents, models) is highly recommended before diving into LangGraph.\\n\\n**Google Vertex AI Agent Builder (A2A):**\\n\\n*   **Focus:** Building conversational AI agents with a visual, low-code/no-code interface. It simplifies the process of creating agents that can understand user intents, extract entities, and fulfill requests.\\n*   **Use Cases:** Well-suited for building:\\n    *   Customer service chatbots.\\n    *   Virtual assistants for specific tasks (e.g., booking appointments, ordering food).\\n    *   Simple conversational interfaces for accessing data or services.\\n*   **Learning Curve:** Easier to get started with. The visual interface and pre-built components make it relatively quick to create a basic agent.\\n*   **Prerequisites:** Less demanding than LangGraph. Familiarity with basic chatbot concepts and natural language understanding is helpful.\\n\\n**Which to Choose First?**\\n\\nHere's my recommendation, based on your current knowledge and goals:\\n\\n*   **If you're new to Langchain and want to see quick results:** Start with **Google Vertex AI Agent Builder (A2A)**. It will give you a taste of building conversational AI applications without requiring a deep dive into complex programming concepts. This can be a good way to build confidence and get a feel for the overall process.\\n\\n*   **If you already have a solid understanding of Langchain fundamentals and want to build more sophisticated and robust applications:** Start with **LangGraph**. It will provide you with the tools and flexibility you need to create complex workflows and manage the state of your agents effectively.\\n\\n**In summary:**\\n\\n| Feature          | LangGraph                                  | Google Vertex AI Agent Builder (A2A) |\\n|-------------------|--------------------------------------------|---------------------------------------|\\n| **Focus**        | Complex workflow orchestration & state mgmt | Conversational AI agent building      |\\n| **Learning Curve** | Steeper                                    | Easier                                 |\\n| **Prerequisites**  | Strong Langchain knowledge                 | Basic chatbot concepts                |\\n| **Use Cases**      | Complex agents, multi-step tasks           | Customer service, simple assistants   |\\n\\nUltimately, the best choice depends on your specific goals and learning style. Consider what kind of applications you want to build and how much time you're willing to invest in learning.\\n\\nNo matter which you choose, I'm here to help you along the way! Let me know if you have any more questions. Good luck!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--1d686cb9-4d16-4167-bc39-7e65ad5ce577-0', usage_metadata={'input_tokens': 221, 'output_tokens': 778, 'total_tokens': 999, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history_cwbm.invoke(\n",
    "    {\n",
    "        \"query\": \"My plan is to learn Langgraph and Google A2A, which one should I go\"  \n",
    "    },\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"session_id\" : \"id_k3\",\n",
    "            \"k\" : 3\n",
    "        }\n",
    "    }\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ccac493d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Okay, that's a great plan! Both LangGraph and Google's A2A (presumably you mean Vertex AI's Agent Builder, which is sometimes referred to as A2A) are powerful tools for building AI applications, but they serve different purposes and have different learning curves. Here's a breakdown to help you decide which to tackle first:\\n\\n**LangGraph:**\\n\\n*   **Focus:** Orchestration and state management for complex, multi-actor LLM applications. Think of it as a framework for building robust and reliable workflows involving multiple LLM calls, conditional logic, and state transitions.\\n*   **Use Cases:** Ideal for applications that require complex decision-making, iterative refinement, and handling of different scenarios. Examples include:\\n    *   Complex chatbots with multiple tools and conversational flows.\\n    *   Automated agents that perform tasks involving multiple steps and dependencies.\\n    *   Applications that require reasoning and planning.\\n*   **Learning Curve:** Steeper initially. You'll need to understand the concepts of graphs, nodes, edges, and state management. However, once you grasp the fundamentals, it provides a very powerful and flexible way to structure your applications.\\n*   **Prerequisites:** A good understanding of Langchain fundamentals (chains, agents, models) is highly recommended before diving into LangGraph.\\n\\n**Google Vertex AI Agent Builder (A2A):**\\n\\n*   **Focus:** Building conversational AI agents with a visual, low-code/no-code interface. It simplifies the process of creating agents that can understand user intents, extract entities, and fulfill requests.\\n*   **Use Cases:** Well-suited for building:\\n    *   Customer service chatbots.\\n    *   Virtual assistants for specific tasks (e.g., booking appointments, ordering food).\\n    *   Simple conversational interfaces for accessing data or services.\\n*   **Learning Curve:** Easier to get started with. The visual interface and pre-built components make it relatively quick to create a basic agent.\\n*   **Prerequisites:** Less demanding than LangGraph. Familiarity with basic chatbot concepts and natural language understanding is helpful.\\n\\n**Which to Choose First?**\\n\\nHere's my recommendation, based on your current knowledge and goals:\\n\\n*   **If you're new to Langchain and want to see quick results:** Start with **Google Vertex AI Agent Builder (A2A)**. It will give you a taste of building conversational AI applications without requiring a deep dive into complex programming concepts. This can be a good way to build confidence and get a feel for the overall process.\\n\\n*   **If you already have a solid understanding of Langchain fundamentals and want to build more sophisticated and robust applications:** Start with **LangGraph**. It will provide you with the tools and flexibility you need to create complex workflows and manage the state of your agents effectively.\\n\\n**In summary:**\\n\\n| Feature          | LangGraph                                  | Google Vertex AI Agent Builder (A2A) |\\n|-------------------|--------------------------------------------|---------------------------------------|\\n| **Focus**        | Complex workflow orchestration & state mgmt | Conversational AI agent building      |\\n| **Learning Curve** | Steeper                                    | Easier                                 |\\n| **Prerequisites**  | Strong Langchain knowledge                 | Basic chatbot concepts                |\\n| **Use Cases**      | Complex agents, multi-step tasks           | Customer service, simple assistants   |\\n\\nUltimately, the best choice depends on your specific goals and learning style. Consider what kind of applications you want to build and how much time you're willing to invest in learning.\\n\\nNo matter which you choose, I'm here to help you along the way! Let me know if you have any more questions. Good luck!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--1d686cb9-4d16-4167-bc39-7e65ad5ce577-0', usage_metadata={'input_tokens': 221, 'output_tokens': 778, 'total_tokens': 999, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_k3\"].messages[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79c6a698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d5a16a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
