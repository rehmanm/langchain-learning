{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1738225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "  langchain-core \\\n",
    "  langchain-google-genai \\\n",
    "  langchain-community \\\n",
    "  scikit-image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8f8a346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter API key for Google Gemini: ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28f10416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "\n",
    "llm = GoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c23e2ac",
   "metadata": {},
   "source": [
    "## Conversation Buffer Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b210e0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2375/1448044083.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51d5e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(\n",
    "    {\"input\": \"Hi, my names in James\"},\n",
    "    {\"output\": \"Hey James, what's up? I'm an AI model called Zeta.\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I am researching different types of conversational memory.\"},\n",
    "    {\"output\": \"That's interesting, what are some examples?\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I am researching ConversationBufferMemory and ConversationBufferWindowMemory\"},\n",
    "    {\"output\": \"That's interesting, what's thed difference\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer memory just store the enitre converation, right?\"},\n",
    "    {\"output\": \"Thats makes sense, what about ConversationBufferWindowMemory\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer window memory stores last k messages, dropping the rest.\"},\n",
    "    {\"output\": \"Very cool!\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5c404e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, my names in James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='I am researching different types of conversational memory.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='I am researching ConversationBufferMemory and ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's thed difference\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just store the enitre converation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Thats makes sense, what about ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f173409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2375/2943370833.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d7fe9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my names in James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='I am researching different types of conversational memory.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='I am researching ConversationBufferMemory and ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's thed difference\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just store the enitre converation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='Thats makes sense, what about ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
      "Human: What is my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name again?',\n",
       " 'history': [HumanMessage(content='Hi, my names in James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='I am researching different types of conversational memory.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='I am researching ConversationBufferMemory and ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's thed difference\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just store the enitre converation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Thats makes sense, what about ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is my name again?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Your name is James.', additional_kwargs={}, response_metadata={})],\n",
       " 'response': 'Your name is James.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is my name again?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c4b963",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory with RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beb6b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant called Zeta.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\")   \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e56847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = (\n",
    "    { \n",
    "        \"query\": lambda x: x[\"query\"],\n",
    "        \"history\": lambda x: x[\"history\"]\n",
    "    }\n",
    "    | prompt_template\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f01f42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chat_map = {}\n",
    "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d19e8257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09185e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello James, it's nice to meet you. My name is Zeta. How can I help you today?\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00b09506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is James.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc00a359",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de8f1a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2375/3216785012.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=4, return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=4, return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c550165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(\n",
    "    {\"input\": \"Hi, my names in James\"},\n",
    "    {\"output\": \"Hey James, what's up? I'm an AI model called Zeta.\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I am researching different types of conversational memory.\"},\n",
    "    {\"output\": \"That's interesting, what are some examples?\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I am researching ConversationBufferMemory and ConversationBufferWindowMemory\"},\n",
    "    {\"output\": \"That's interesting, what's thed difference\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer memory just store the enitre converation, right?\"},\n",
    "    {\"output\": \"Thats makes sense, what about ConversationBufferWindowMemory\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer window memory stores last k messages, dropping the rest.\"},\n",
    "    {\"output\": \"Very cool!\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "276af513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='I am researching different types of conversational memory.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='I am researching ConversationBufferMemory and ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's thed difference\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just store the enitre converation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Thats makes sense, what about ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a9ed9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dd5e474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='I am researching different types of conversational memory.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='I am researching ConversationBufferMemory and ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's thed difference\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just store the enitre converation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='Thats makes sense, what about ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
      "Human: What is my name again\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name again',\n",
       " 'history': [HumanMessage(content='I am researching different types of conversational memory.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='I am researching ConversationBufferMemory and ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's thed difference\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just store the enitre converation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Thats makes sense, what about ConversationBufferWindowMemory', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})],\n",
       " 'response': 'I am sorry, I do not have access to that information. I do not have access to your name.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is my name again\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6809097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    k: int = Field(default_factory=int)\n",
    "    \n",
    "    \n",
    "    def __init__(self, k:int):\n",
    "        super().__init__(k=k)\n",
    "        print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
    "        \n",
    "    \n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"\n",
    "        Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages\n",
    "        \"\"\"\n",
    "        \n",
    "        self.messages.extend(messages)\n",
    "        self.messages = self.messages[-self.k:]\n",
    "        \n",
    "        \n",
    "    def clear(self) -> None:\n",
    "        \"\"\"\n",
    "        clear the history\n",
    "        \"\"\"\n",
    "        self.messages = []       \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0653e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "def get_chat_window_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
    "    \n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "115022d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "pipeline_cwbm = (\n",
    "    { \n",
    "        \"query\": lambda x: x[\"query\"],\n",
    "        \"history\": lambda x: x[\"history\"]\n",
    "    }\n",
    "    | prompt_template\n",
    "    | llm\n",
    ")\n",
    " \n",
    "\n",
    "pipeline_with_history_cwbm = RunnableWithMessageHistory(\n",
    "    pipeline_cwbm,\n",
    "    get_session_history = get_chat_window_history,\n",
    "    input_messages_key = \"query\",\n",
    "    history_messages_key = \"history\",\n",
    "    history_factory_config = [\n",
    "        ConfigurableFieldSpec(\n",
    "            id = \"session_id\",\n",
    "            annotation = str,\n",
    "            name = \"Session ID\",\n",
    "            description = \"The session ID to use for the chat history\",\n",
    "            default = \"id_default\"\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id = \"k\",\n",
    "            annotation = int,\n",
    "            name = \"k\",\n",
    "            description = \"The number of messages to keep in the history\",\n",
    "            default = 4\n",
    "        )\n",
    "    ]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de016f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BufferWindowMessageHistory with k=3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Okay, James. It's nice to meet you. How can I help you today?\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history_cwbm.invoke(\n",
    "    {\n",
    "        \"query\": \"my name is James\"  \n",
    "    },\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"session_id\" : \"id_k3\",\n",
    "            \"k\" : 3\n",
    "        }\n",
    "    }\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de6a9e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"AI: That's great, James! Langchain is a powerful framework. What are you hoping to learn about it? Are you just starting out, or do you have some specific questions or goals in mind?\\n\\nFor example, are you interested in:\\n\\n*   **Understanding the core concepts?** (e.g., LLMs, Prompts, Chains, Agents, Memory)\\n*   **Building a specific type of application?** (e.g., Question Answering, Chatbots, Document Summarization)\\n*   **Working with a particular LLM?** (e.g., OpenAI, Hugging Face, Cohere)\\n*   **Troubleshooting a specific issue?**\\n\\nThe more information you give me, the better I can assist you. Let me know what you'd like to focus on!\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history_cwbm.invoke(\n",
    "    {\n",
    "        \"query\": \"I am learning langchain\"  \n",
    "    },\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"session_id\" : \"id_k3\",\n",
    "            \"k\" : 3\n",
    "        }\n",
    "    }\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7c51b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI: Okay, James, that\\'s a great plan! Both LangGraph and Google\\'s A2A (Agent-to-Agent) communication are exciting areas. To help you decide where to start, let\\'s break down the key differences and considerations:\\n\\n**LangGraph:**\\n\\n*   **Focus:** Orchestration and building complex, stateful, multi-actor workflows using LLMs. Think of it as a way to define a graph-like structure where different LLM-powered \"nodes\" (agents, tools, or functions) interact and pass information to each other.\\n*   **Key Features:**\\n    *   **Graph-based structure:**  Define the flow of information and control between different components.\\n    *   **State management:**  Maintain state across multiple steps in the workflow.\\n    *   **Conditional routing:**  Route the flow based on the output of previous steps.\\n    *   **Integration with LangChain:**  Built on top of LangChain, so you can leverage existing LangChain components.\\n*   **Use Cases:**\\n    *   Complex agent interactions (e.g., a sales agent collaborating with a support agent).\\n    *   Multi-step reasoning and planning.\\n    *   Building custom workflows for specific tasks.\\n*   **Learning Curve:**  Requires understanding of graph data structures and state management concepts, in addition to LangChain basics.\\n\\n**Google A2A (Agent-to-Agent Communication):**\\n\\n*   **Focus:**  Enabling multiple agents to communicate and collaborate to solve complex problems.  This is a broader concept than LangGraph, and Google\\'s implementation is likely a specific framework or set of tools for achieving this.\\n*   **Key Features (General A2A):**\\n    *   **Message passing:** Agents exchange information through messages.\\n    *   **Coordination mechanisms:**  Protocols for agents to agree on tasks, share resources, and resolve conflicts.\\n    *   **Knowledge sharing:**  Agents can share their knowledge and expertise with each other.\\n    *   **Role assignment:**  Agents can be assigned specific roles and responsibilities.\\n*   **Use Cases:**\\n    *   Distributed problem solving.\\n    *   Collaborative task completion.\\n    *   Building intelligent teams of agents.\\n*   **Learning Curve:**  Depends on the specific A2A framework.  Generally, it involves understanding communication protocols, agent architectures, and coordination strategies.\\n\\n**Which should you learn first?**\\n\\nHere\\'s my recommendation, considering you\\'re learning LangChain:\\n\\n1.  **LangGraph First:**  Since LangGraph is built on LangChain, it\\'s a natural progression.  It provides a concrete framework for building multi-agent workflows.  Understanding LangGraph will give you a solid foundation for understanding the challenges and opportunities of A2A communication.  You\\'ll also be able to leverage your existing LangChain knowledge.\\n\\n2.  **Then, Google A2A:**  Once you have a good grasp of LangGraph, you can explore Google\\'s A2A framework (if it\\'s publicly available and well-documented).  You\\'ll be better equipped to understand its specific features and how it compares to other A2A approaches.  You can then consider how Google\\'s A2A might offer different capabilities or advantages compared to LangGraph for certain use cases.\\n\\n**Why this order?**\\n\\n*   **Gradual Learning:** LangGraph builds on your existing LangChain knowledge.\\n*   **Practical Experience:** LangGraph provides a hands-on way to build multi-agent systems.\\n*   **Context for A2A:** Understanding LangGraph will give you a better understanding of the challenges and design considerations in A2A communication.\\n\\n**Before diving into Google A2A, make sure:**\\n\\n*   **It\\'s publicly available:**  Check if Google has released documentation, code, or tutorials for their A2A framework.\\n*   **It\\'s well-documented:**  Good documentation is essential for learning a new framework.\\n*   **It aligns with your goals:**  Consider whether Google\\'s A2A framework is a good fit for the types of applications you want to build.\\n\\nIn summary, start with LangGraph to build a solid foundation in multi-agent workflows within the LangChain ecosystem. Then, explore Google\\'s A2A framework to broaden your understanding of A2A communication and potentially leverage its specific capabilities. Good luck! Let me know if you have any more questions.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history_cwbm.invoke(\n",
    "    {\n",
    "        \"query\": \"My plan is to learn Langgraph and Google A2A, which one should I go\"  \n",
    "    },\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"session_id\" : \"id_k3\",\n",
    "            \"k\" : 3\n",
    "        }\n",
    "    }\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b6a4030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='AI: Okay, James, that\\'s a great plan! Both LangGraph and Google\\'s A2A (Agent-to-Agent) communication are exciting areas. To help you decide where to start, let\\'s break down the key differences and considerations:\\n\\n**LangGraph:**\\n\\n*   **Focus:** Orchestration and building complex, stateful, multi-actor workflows using LLMs. Think of it as a way to define a graph-like structure where different LLM-powered \"nodes\" (agents, tools, or functions) interact and pass information to each other.\\n*   **Key Features:**\\n    *   **Graph-based structure:**  Define the flow of information and control between different components.\\n    *   **State management:**  Maintain state across multiple steps in the workflow.\\n    *   **Conditional routing:**  Route the flow based on the output of previous steps.\\n    *   **Integration with LangChain:**  Built on top of LangChain, so you can leverage existing LangChain components.\\n*   **Use Cases:**\\n    *   Complex agent interactions (e.g., a sales agent collaborating with a support agent).\\n    *   Multi-step reasoning and planning.\\n    *   Building custom workflows for specific tasks.\\n*   **Learning Curve:**  Requires understanding of graph data structures and state management concepts, in addition to LangChain basics.\\n\\n**Google A2A (Agent-to-Agent Communication):**\\n\\n*   **Focus:**  Enabling multiple agents to communicate and collaborate to solve complex problems.  This is a broader concept than LangGraph, and Google\\'s implementation is likely a specific framework or set of tools for achieving this.\\n*   **Key Features (General A2A):**\\n    *   **Message passing:** Agents exchange information through messages.\\n    *   **Coordination mechanisms:**  Protocols for agents to agree on tasks, share resources, and resolve conflicts.\\n    *   **Knowledge sharing:**  Agents can share their knowledge and expertise with each other.\\n    *   **Role assignment:**  Agents can be assigned specific roles and responsibilities.\\n*   **Use Cases:**\\n    *   Distributed problem solving.\\n    *   Collaborative task completion.\\n    *   Building intelligent teams of agents.\\n*   **Learning Curve:**  Depends on the specific A2A framework.  Generally, it involves understanding communication protocols, agent architectures, and coordination strategies.\\n\\n**Which should you learn first?**\\n\\nHere\\'s my recommendation, considering you\\'re learning LangChain:\\n\\n1.  **LangGraph First:**  Since LangGraph is built on LangChain, it\\'s a natural progression.  It provides a concrete framework for building multi-agent workflows.  Understanding LangGraph will give you a solid foundation for understanding the challenges and opportunities of A2A communication.  You\\'ll also be able to leverage your existing LangChain knowledge.\\n\\n2.  **Then, Google A2A:**  Once you have a good grasp of LangGraph, you can explore Google\\'s A2A framework (if it\\'s publicly available and well-documented).  You\\'ll be better equipped to understand its specific features and how it compares to other A2A approaches.  You can then consider how Google\\'s A2A might offer different capabilities or advantages compared to LangGraph for certain use cases.\\n\\n**Why this order?**\\n\\n*   **Gradual Learning:** LangGraph builds on your existing LangChain knowledge.\\n*   **Practical Experience:** LangGraph provides a hands-on way to build multi-agent systems.\\n*   **Context for A2A:** Understanding LangGraph will give you a better understanding of the challenges and design considerations in A2A communication.\\n\\n**Before diving into Google A2A, make sure:**\\n\\n*   **It\\'s publicly available:**  Check if Google has released documentation, code, or tutorials for their A2A framework.\\n*   **It\\'s well-documented:**  Good documentation is essential for learning a new framework.\\n*   **It aligns with your goals:**  Consider whether Google\\'s A2A framework is a good fit for the types of applications you want to build.\\n\\nIn summary, start with LangGraph to build a solid foundation in multi-agent workflows within the LangChain ecosystem. Then, explore Google\\'s A2A framework to broaden your understanding of A2A communication and potentially leverage its specific capabilities. Good luck! Let me know if you have any more questions.', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_k3\"].messages[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff34ba35",
   "metadata": {},
   "source": [
    "## Converasation Summary Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a68053a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2375/3825381965.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  summary_memory = ConversationSummaryMemory(llm = llm)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "summary_memory = ConversationSummaryMemory(llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4facf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = summary_memory,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbb45ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human introduces himself as Muhammad. The AI greets Muhammad, explains it doesn't have a name in the human sense but can be called AI, and describes itself as a large language model trained by Google, capable of communicating and generating human-like text. It then asks how it can help Muhammad. Muhammad says he is learning about \"chain\" and exploring its uses. The AI responds enthusiastically, clarifies that \"chain\" could refer to blockchain technology, Markov chains, or supply chain management, and provides a brief overview of each, including potential applications. It then asks Muhammad to specify which type of \"chain\" he is most interested in to provide more relevant information. Muhammad specifies he is looking into ConversationBufferMemory and ConversationWindowBufferMemory. The AI confirms Muhammad is exploring conversational AI and memory management, then provides detailed explanations of ConversationBufferMemory (a running transcript of the entire conversation with pros and cons like complete history vs. memory intensity) and ConversationWindowBufferMemory (keeping only a fixed number of recent interactions with pros and cons like memory efficiency vs. loss of history). It also provides key considerations for choosing between them based on conversation length, context relevance, memory constraints, and token limits, along with example use cases for each. Finally, it asks Muhammad what specific aspects he'd like to explore further. Muhammad confirms that BufferMemory stores the entire conversation. The AI elaborates, confirming that ConversationBufferMemory stores the entire conversation history as a single string, allowing the AI to have a complete understanding of everything that has been said, which can be very useful for maintaining context and providing relevant responses. However, this can become memory-intensive, especially for long conversations, leading to higher API costs, slower response times, and potentially exceeding the token limit of the language model. The AI then asks if Muhammad has any specific questions about how ConversationBufferMemory handles the entire conversation, or how it compares to ConversationWindowBufferMemory in this regard.\n",
      "Human: hello there my name in Muhammad\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human introduces himself as Muhammad. The AI greets Muhammad, explains it doesn't have a name in the human sense but can be called AI, and describes itself as a large language model trained by Google, capable of communicating and generating human-like text. It then asks how it can help Muhammad. Muhammad says he is learning about \"chain\" and exploring its uses. The AI responds enthusiastically, clarifies that \"chain\" could refer to blockchain technology, Markov chains, or supply chain management, and provides a brief overview of each, including potential applications. It then asks Muhammad to specify which type of \"chain\" he is most interested in to provide more relevant information. Muhammad specifies he is looking into ConversationBufferMemory and ConversationWindowBufferMemory. The AI confirms Muhammad is exploring conversational AI and memory management, then provides detailed explanations of ConversationBufferMemory (a running transcript of the entire conversation with pros and cons like complete history vs. memory intensity) and ConversationWindowBufferMemory (keeping only a fixed number of recent interactions with pros and cons like memory efficiency vs. loss of history). It also provides key considerations for choosing between them based on conversation length, context relevance, memory constraints, and token limits, along with example use cases for each. Finally, it asks Muhammad what specific aspects he'd like to explore further. Muhammad confirms that BufferMemory stores the entire conversation. The AI elaborates, confirming that ConversationBufferMemory stores the entire conversation history as a single string, allowing the AI to have a complete understanding of everything that has been said, which can be very useful for maintaining context and providing relevant responses. However, this can become memory-intensive, especially for long conversations, leading to higher API costs, slower response times, and potentially exceeding the token limit of the language model. The AI then asks if Muhammad has any specific questions about how ConversationBufferMemory handles the entire conversation, or how it compares to ConversationWindowBufferMemory in this regard. Muhammad greets the AI again. The AI reiterates that they were discussing ConversationBufferMemory and offers several suggestions for further exploration, including specific use cases, token limit management, comparison with other memory types, and implementation details, asking Muhammad to choose a topic.\n",
      "Human: I am learning chain and exploring how it can be use\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human introduces himself as Muhammad. The AI greets Muhammad, explains it doesn't have a name in the human sense but can be called AI, and describes itself as a large language model trained by Google, capable of communicating and generating human-like text. It then asks how it can help Muhammad. Muhammad says he is learning about \"chain\" and exploring its uses. The AI responds enthusiastically, clarifies that \"chain\" could refer to blockchain technology, Markov chains, or supply chain management, and provides a brief overview of each, including potential applications. It then asks Muhammad to specify which type of \"chain\" he is most interested in to provide more relevant information. Muhammad specifies he is looking into ConversationBufferMemory and ConversationWindowBufferMemory. The AI confirms Muhammad is exploring conversational AI and memory management, then provides detailed explanations of ConversationBufferMemory (a running transcript of the entire conversation with pros and cons like complete history vs. memory intensity) and ConversationWindowBufferMemory (keeping only a fixed number of recent interactions with pros and cons like memory efficiency vs. loss of history). It also provides key considerations for choosing between them based on conversation length, context relevance, memory constraints, and token limits, along with example use cases for each. Finally, it asks Muhammad what specific aspects he'd like to explore further. Muhammad confirms that BufferMemory stores the entire conversation. The AI elaborates, confirming that ConversationBufferMemory stores the entire conversation history as a single string, allowing the AI to have a complete understanding of everything that has been said, which can be very useful for maintaining context and providing relevant responses. However, this can become memory-intensive, especially for long conversations, leading to higher API costs, slower response times, and potentially exceeding the token limit of the language model. The AI then asks if Muhammad has any specific questions about how ConversationBufferMemory handles the entire conversation, or how it compares to ConversationWindowBufferMemory in this regard. Muhammad greets the AI again. The AI reiterates that they were discussing ConversationBufferMemory and offers several suggestions for further exploration, including specific use cases, token limit management, comparison with other memory types, and implementation details, asking Muhammad to choose a topic. Muhammad states he is learning chain and exploring how it can be used. The AI welcomes Muhammad back, recaps their discussion of ConversationBufferMemory and its storage of the entire conversation history, and asks about the specific use case Muhammad has in mind (e.g., customer service chatbot, personal assistant, dynamic dialogue game) to tailor its response. It then offers several potential avenues for further exploration, including specific examples, practical applications, limitations and mitigation strategies, and comparison with other memory types, asking Muhammad to choose a topic.\n",
      "Human: I am looking into ConversationBufferMemory and ConversationWindowBufferMemory\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human introduces himself as Muhammad. The AI greets Muhammad, explains it doesn't have a name in the human sense but can be called AI, and describes itself as a large language model trained by Google, capable of communicating and generating human-like text. It then asks how it can help Muhammad. Muhammad says he is learning about \"chain\" and exploring its uses. The AI responds enthusiastically, clarifies that \"chain\" could refer to blockchain technology, Markov chains, or supply chain management, and provides a brief overview of each, including potential applications. It then asks Muhammad to specify which type of \"chain\" he is most interested in to provide more relevant information. Muhammad specifies he is looking into ConversationBufferMemory and ConversationWindowBufferMemory. The AI confirms Muhammad is exploring conversational AI and memory management, then provides detailed explanations of ConversationBufferMemory (a running transcript of the entire conversation with pros and cons like complete history vs. memory intensity) and ConversationWindowBufferMemory (keeping only a fixed number of recent interactions with pros and cons like memory efficiency vs. loss of history). It also provides key considerations for choosing between them based on conversation length, context relevance, memory constraints, and token limits, along with example use cases for each. Finally, it asks Muhammad what specific aspects he'd like to explore further. Muhammad confirms that BufferMemory stores the entire conversation. The AI elaborates, confirming that ConversationBufferMemory stores the entire conversation history as a single string, allowing the AI to have a complete understanding of everything that has been said, which can be very useful for maintaining context and providing relevant responses. However, this can become memory-intensive, especially for long conversations, leading to higher API costs, slower response times, and potentially exceeding the token limit of the language model. The AI then asks if Muhammad has any specific questions about how ConversationBufferMemory handles the entire conversation, or how it compares to ConversationWindowBufferMemory in this regard. Muhammad greets the AI again. The AI reiterates that they were discussing ConversationBufferMemory and offers several suggestions for further exploration, including specific use cases, token limit management, comparison with other memory types, and implementation details, asking Muhammad to choose a topic. Muhammad states he is learning chain and exploring how it can be used. The AI welcomes Muhammad back, recaps their discussion of ConversationBufferMemory and its storage of the entire conversation history, and asks about the specific use case Muhammad has in mind (e.g., customer service chatbot, personal assistant, dynamic dialogue game) to tailor its response. It then offers several potential avenues for further exploration, including specific examples, practical applications, limitations and mitigation strategies, and comparison with other memory types, asking Muhammad to choose a topic. Muhammad states he is looking into ConversationBufferMemory and ConversationWindowBufferMemory. The AI acknowledges they've discussed this before, recaps the key differences between ConversationBufferMemory (stores entire history, good context but memory-intensive) and ConversationWindowBufferMemory (stores recent interactions, memory-efficient but loses older context), and asks Muhammad to specify what aspects he wants to explore further and if he has a specific application in mind.\n",
      "Human: BufferMemory stores the entire conversation\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human introduces himself as Muhammad. The AI greets Muhammad, explains it doesn't have a name in the human sense but can be called AI, and describes itself as a large language model trained by Google, capable of communicating and generating human-like text. It then asks how it can help Muhammad. Muhammad says he is learning about \"chain\" and exploring its uses. The AI responds enthusiastically, clarifies that \"chain\" could refer to blockchain technology, Markov chains, or supply chain management, and provides a brief overview of each, including potential applications. It then asks Muhammad to specify which type of \"chain\" he is most interested in to provide more relevant information. Muhammad specifies he is looking into ConversationBufferMemory and ConversationWindowBufferMemory. The AI confirms Muhammad is exploring conversational AI and memory management, then provides detailed explanations of ConversationBufferMemory (a running transcript of the entire conversation with pros and cons like complete history vs. memory intensity) and ConversationWindowBufferMemory (keeping only a fixed number of recent interactions with pros and cons like memory efficiency vs. loss of history). It also provides key considerations for choosing between them based on conversation length, context relevance, memory constraints, and token limits, along with example use cases for each. Finally, it asks Muhammad what specific aspects he'd like to explore further. Muhammad confirms that BufferMemory stores the entire conversation. The AI elaborates, confirming that ConversationBufferMemory stores the entire conversation history as a single string, allowing the AI to have a complete understanding of everything that has been said, which can be very useful for maintaining context and providing relevant responses. However, this can become memory-intensive, especially for long conversations, leading to higher API costs, slower response times, and potentially exceeding the token limit of the language model. The AI then asks if Muhammad has any specific questions about how ConversationBufferMemory handles the entire conversation, or how it compares to ConversationWindowBufferMemory in this regard. Muhammad greets the AI again. The AI reiterates that they were discussing ConversationBufferMemory and offers several suggestions for further exploration, including specific use cases, token limit management, comparison with other memory types, and implementation details, asking Muhammad to choose a topic. Muhammad states he is learning chain and exploring how it can be used. The AI welcomes Muhammad back, recaps their discussion of ConversationBufferMemory and its storage of the entire conversation history, and asks about the specific use case Muhammad has in mind (e.g., customer service chatbot, personal assistant, dynamic dialogue game) to tailor its response. It then offers several potential avenues for further exploration, including specific examples, practical applications, limitations and mitigation strategies, and comparison with other memory types, asking Muhammad to choose a topic. Muhammad states he is looking into ConversationBufferMemory and ConversationWindowBufferMemory. The AI acknowledges they've discussed this before, recaps the key differences between ConversationBufferMemory (stores entire history, good context but memory-intensive) and ConversationWindowBufferMemory (stores recent interactions, memory-efficient but loses older context), and asks Muhammad to specify what aspects he wants to explore further and if he has a specific application in mind. Muhammad confirms that BufferMemory stores the entire conversation. The AI affirms this, reiterating that ConversationBufferMemory stores the entire conversation history as a single string, which allows the AI to maintain context, personalize interactions, and perform complex reasoning. However, this comes at the cost of increased memory usage, higher API costs, slower response times, and potential token limit issues. The AI then asks Muhammad what specific aspects of ConversationBufferMemory's storage of the entire conversation he would like to explore further, suggesting examples, token limit strategies, comparisons with ConversationWindowBufferMemory, and implementation details.\n",
      "Human: BufferWindowMemory stores last k messages and dropped previous messages\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'BufferWindowMemory stores last k messages and dropped previous messages',\n",
       " 'history': 'The human introduces himself as Muhammad. The AI greets Muhammad, explains it doesn\\'t have a name in the human sense but can be called AI, and describes itself as a large language model trained by Google, capable of communicating and generating human-like text. It then asks how it can help Muhammad. Muhammad says he is learning about \"chain\" and exploring its uses. The AI responds enthusiastically, clarifies that \"chain\" could refer to blockchain technology, Markov chains, or supply chain management, and provides a brief overview of each, including potential applications. It then asks Muhammad to specify which type of \"chain\" he is most interested in to provide more relevant information. Muhammad specifies he is looking into ConversationBufferMemory and ConversationWindowBufferMemory. The AI confirms Muhammad is exploring conversational AI and memory management, then provides detailed explanations of ConversationBufferMemory (a running transcript of the entire conversation with pros and cons like complete history vs. memory intensity) and ConversationWindowBufferMemory (keeping only a fixed number of recent interactions with pros and cons like memory efficiency vs. loss of history). It also provides key considerations for choosing between them based on conversation length, context relevance, memory constraints, and token limits, along with example use cases for each. Finally, it asks Muhammad what specific aspects he\\'d like to explore further. Muhammad confirms that BufferMemory stores the entire conversation. The AI elaborates, confirming that ConversationBufferMemory stores the entire conversation history as a single string, allowing the AI to have a complete understanding of everything that has been said, which can be very useful for maintaining context and providing relevant responses. However, this can become memory-intensive, especially for long conversations, leading to higher API costs, slower response times, and potentially exceeding the token limit of the language model. The AI then asks if Muhammad has any specific questions about how ConversationBufferMemory handles the entire conversation, or how it compares to ConversationWindowBufferMemory in this regard. Muhammad greets the AI again. The AI reiterates that they were discussing ConversationBufferMemory and offers several suggestions for further exploration, including specific use cases, token limit management, comparison with other memory types, and implementation details, asking Muhammad to choose a topic. Muhammad states he is learning chain and exploring how it can be used. The AI welcomes Muhammad back, recaps their discussion of ConversationBufferMemory and its storage of the entire conversation history, and asks about the specific use case Muhammad has in mind (e.g., customer service chatbot, personal assistant, dynamic dialogue game) to tailor its response. It then offers several potential avenues for further exploration, including specific examples, practical applications, limitations and mitigation strategies, and comparison with other memory types, asking Muhammad to choose a topic. Muhammad states he is looking into ConversationBufferMemory and ConversationWindowBufferMemory. The AI acknowledges they\\'ve discussed this before, recaps the key differences between ConversationBufferMemory (stores entire history, good context but memory-intensive) and ConversationWindowBufferMemory (stores recent interactions, memory-efficient but loses older context), and asks Muhammad to specify what aspects he wants to explore further and if he has a specific application in mind. Muhammad confirms that BufferMemory stores the entire conversation. The AI affirms this, reiterating that ConversationBufferMemory stores the entire conversation history as a single string, which allows the AI to maintain context, personalize interactions, and perform complex reasoning. However, this comes at the cost of increased memory usage, higher API costs, slower response times, and potential token limit issues. The AI then asks Muhammad what specific aspects of ConversationBufferMemory\\'s storage of the entire conversation he would like to explore further, suggesting examples, token limit strategies, comparisons with ConversationWindowBufferMemory, and implementation details.',\n",
       " 'response': 'That\\'s absolutely correct, Muhammad! You\\'ve got the core concept of `ConversationWindowBufferMemory` down.\\n\\nTo reiterate and expand slightly: `ConversationWindowBufferMemory` stores only the *k* most recent messages (or interactions) in the conversation. When a new message comes in, the oldest message is dropped to maintain the fixed window size. This \"window\" slides forward as the conversation progresses.\\n\\nThis approach offers several advantages:\\n\\n*   **Memory Efficiency:** By limiting the number of stored messages, it significantly reduces memory consumption compared to `ConversationBufferMemory`. This is crucial for applications with limited resources or long-running conversations.\\n*   **Cost-Effective:** Less memory usage translates to lower API costs, especially when using cloud-based language models that charge based on token usage.\\n*   **Faster Response Times:** Processing a smaller amount of conversation history leads to quicker response generation.\\n*   **Token Limit Management:** It helps to avoid exceeding the token limit of the language model, which can prevent errors or truncation of responses.\\n\\nHowever, it also has a key disadvantage:\\n\\n*   **Loss of Context:** By discarding older messages, the AI loses access to potentially relevant information from earlier in the conversation. This can impact its ability to understand the full context, personalize interactions, or perform complex reasoning that requires a broader understanding of the conversation history.\\n\\nSo, to summarize, `ConversationWindowBufferMemory` is a good choice when memory efficiency, cost, and speed are paramount, and when the context required for each response is primarily based on recent interactions.\\n\\nDo you have any specific questions about how `ConversationWindowBufferMemory` handles the dropping of older messages, or perhaps how to determine the optimal window size (*k*) for a particular application? We could also explore strategies for mitigating the loss of context, or compare it to other memory types in more detail. What would you like to delve into next?'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_chain.invoke({\"input\": \"hello there my name in Muhammad\"})\n",
    "summary_chain.invoke({\"input\": \"I am learning chain and exploring how it can be use\"})\n",
    "summary_chain.invoke({\"input\": \"I am looking into ConversationBufferMemory and ConversationWindowBufferMemory\"})\n",
    "summary_chain.invoke({\"input\": \"BufferMemory stores the entire conversation\"})\n",
    "summary_chain.invoke({\"input\": \"BufferWindowMemory stores last k messages and dropped previous messages\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d888de88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human introduces himself as Muhammad. The AI greets Muhammad, explains it doesn't have a name in the human sense but can be called AI, and describes itself as a large language model trained by Google, capable of communicating and generating human-like text. It then asks how it can help Muhammad. Muhammad says he is learning about \"chain\" and exploring its uses. The AI responds enthusiastically, clarifies that \"chain\" could refer to blockchain technology, Markov chains, or supply chain management, and provides a brief overview of each, including potential applications. It then asks Muhammad to specify which type of \"chain\" he is most interested in to provide more relevant information. Muhammad specifies he is looking into ConversationBufferMemory and ConversationWindowBufferMemory. The AI confirms Muhammad is exploring conversational AI and memory management, then provides detailed explanations of ConversationBufferMemory (a running transcript of the entire conversation with pros and cons like complete history vs. memory intensity) and ConversationWindowBufferMemory (keeping only a fixed number of recent interactions with pros and cons like memory efficiency vs. loss of history). It also provides key considerations for choosing between them based on conversation length, context relevance, memory constraints, and token limits, along with example use cases for each. Finally, it asks Muhammad what specific aspects he'd like to explore further. Muhammad confirms that BufferMemory stores the entire conversation. The AI elaborates, confirming that ConversationBufferMemory stores the entire conversation history as a single string, allowing the AI to have a complete understanding of everything that has been said, which can be very useful for maintaining context and providing relevant responses. However, this can become memory-intensive, especially for long conversations, leading to higher API costs, slower response times, and potentially exceeding the token limit of the language model. The AI then asks if Muhammad has any specific questions about how ConversationBufferMemory handles the entire conversation, or how it compares to ConversationWindowBufferMemory in this regard. Muhammad greets the AI again. The AI reiterates that they were discussing ConversationBufferMemory and offers several suggestions for further exploration, including specific use cases, token limit management, comparison with other memory types, and implementation details, asking Muhammad to choose a topic. Muhammad states he is learning chain and exploring how it can be used. The AI welcomes Muhammad back, recaps their discussion of ConversationBufferMemory and its storage of the entire conversation history, and asks about the specific use case Muhammad has in mind (e.g., customer service chatbot, personal assistant, dynamic dialogue game) to tailor its response. It then offers several potential avenues for further exploration, including specific examples, practical applications, limitations and mitigation strategies, and comparison with other memory types, asking Muhammad to choose a topic. Muhammad states he is looking into ConversationBufferMemory and ConversationWindowBufferMemory. The AI acknowledges they've discussed this before, recaps the key differences between ConversationBufferMemory (stores entire history, good context but memory-intensive) and ConversationWindowBufferMemory (stores recent interactions, memory-efficient but loses older context), and asks Muhammad to specify what aspects he wants to explore further and if he has a specific application in mind. Muhammad confirms that BufferMemory stores the entire conversation. The AI affirms this, reiterating that ConversationBufferMemory stores the entire conversation history as a single string, which allows the AI to maintain context, personalize interactions, and perform complex reasoning. However, this comes at the cost of increased memory usage, higher API costs, slower response times, and potential token limit issues. The AI then asks Muhammad what specific aspects of ConversationBufferMemory's storage of the entire conversation he would like to explore further, suggesting examples, token limit strategies, comparisons with ConversationWindowBufferMemory, and implementation details. Muhammad states BufferWindowMemory stores last k messages and dropped previous messages. The AI confirms Muhammad's understanding of ConversationWindowBufferMemory, reiterating that it stores only the *k* most recent messages, dropping older ones to maintain a fixed window size. It then details the advantages (memory efficiency, cost-effectiveness, faster response times, token limit management) and disadvantages (loss of context) of this approach, summarizing that it's a good choice when recent context is most important and memory is limited. Finally, it asks Muhammad what specific aspects he'd like to explore further, such as how older messages are dropped, determining the optimal window size, mitigating context loss, or comparing it to other memory types.\n",
      "Human: What is my name\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name',\n",
       " 'history': 'The human introduces himself as Muhammad. The AI greets Muhammad, explains it doesn\\'t have a name in the human sense but can be called AI, and describes itself as a large language model trained by Google, capable of communicating and generating human-like text. It then asks how it can help Muhammad. Muhammad says he is learning about \"chain\" and exploring its uses. The AI responds enthusiastically, clarifies that \"chain\" could refer to blockchain technology, Markov chains, or supply chain management, and provides a brief overview of each, including potential applications. It then asks Muhammad to specify which type of \"chain\" he is most interested in to provide more relevant information. Muhammad specifies he is looking into ConversationBufferMemory and ConversationWindowBufferMemory. The AI confirms Muhammad is exploring conversational AI and memory management, then provides detailed explanations of ConversationBufferMemory (a running transcript of the entire conversation with pros and cons like complete history vs. memory intensity) and ConversationWindowBufferMemory (keeping only a fixed number of recent interactions with pros and cons like memory efficiency vs. loss of history). It also provides key considerations for choosing between them based on conversation length, context relevance, memory constraints, and token limits, along with example use cases for each. Finally, it asks Muhammad what specific aspects he\\'d like to explore further. Muhammad confirms that BufferMemory stores the entire conversation. The AI elaborates, confirming that ConversationBufferMemory stores the entire conversation history as a single string, allowing the AI to have a complete understanding of everything that has been said, which can be very useful for maintaining context and providing relevant responses. However, this can become memory-intensive, especially for long conversations, leading to higher API costs, slower response times, and potentially exceeding the token limit of the language model. The AI then asks if Muhammad has any specific questions about how ConversationBufferMemory handles the entire conversation, or how it compares to ConversationWindowBufferMemory in this regard. Muhammad greets the AI again. The AI reiterates that they were discussing ConversationBufferMemory and offers several suggestions for further exploration, including specific use cases, token limit management, comparison with other memory types, and implementation details, asking Muhammad to choose a topic. Muhammad states he is learning chain and exploring how it can be used. The AI welcomes Muhammad back, recaps their discussion of ConversationBufferMemory and its storage of the entire conversation history, and asks about the specific use case Muhammad has in mind (e.g., customer service chatbot, personal assistant, dynamic dialogue game) to tailor its response. It then offers several potential avenues for further exploration, including specific examples, practical applications, limitations and mitigation strategies, and comparison with other memory types, asking Muhammad to choose a topic. Muhammad states he is looking into ConversationBufferMemory and ConversationWindowBufferMemory. The AI acknowledges they\\'ve discussed this before, recaps the key differences between ConversationBufferMemory (stores entire history, good context but memory-intensive) and ConversationWindowBufferMemory (stores recent interactions, memory-efficient but loses older context), and asks Muhammad to specify what aspects he wants to explore further and if he has a specific application in mind. Muhammad confirms that BufferMemory stores the entire conversation. The AI affirms this, reiterating that ConversationBufferMemory stores the entire conversation history as a single string, which allows the AI to maintain context, personalize interactions, and perform complex reasoning. However, this comes at the cost of increased memory usage, higher API costs, slower response times, and potential token limit issues. The AI then asks Muhammad what specific aspects of ConversationBufferMemory\\'s storage of the entire conversation he would like to explore further, suggesting examples, token limit strategies, comparisons with ConversationWindowBufferMemory, and implementation details. Muhammad states BufferWindowMemory stores last k messages and dropped previous messages. The AI confirms Muhammad\\'s understanding of ConversationWindowBufferMemory, reiterating that it stores only the *k* most recent messages, dropping older ones to maintain a fixed window size. It then details the advantages (memory efficiency, cost-effectiveness, faster response times, token limit management) and disadvantages (loss of context) of this approach, summarizing that it\\'s a good choice when recent context is most important and memory is limited. Finally, it asks Muhammad what specific aspects he\\'d like to explore further, such as how older messages are dropped, determining the optimal window size, mitigating context loss, or comparing it to other memory types.',\n",
       " 'response': \"Ah, a good question! Based on our previous interactions, your name is Muhammad. Is there anything else I can help you with, Muhammad? Perhaps you'd like to delve deeper into ConversationWindowBufferMemory or ConversationBufferMemory?\"}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_chain.invoke({\"input\": \"What is my name\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439167eb",
   "metadata": {},
   "source": [
    "## Conversation Summary Memory with Runnable Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8617c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0afec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel): \n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: GoogleGenerativeAI = Field(default_factory=GoogleGenerativeAI)\n",
    "    \n",
    "    def __init__(self, llm:GoogleGenerativeAI):\n",
    "        super().__init__(llm=llm)\n",
    "    \n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history\n",
    "        \"\"\"\n",
    "        self.messages.extend(messages)\n",
    "        # construct the summary chat messages\n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"Given the existing conversation summary and the new messages, \"\n",
    "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                \"as much relevant information as possible.\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
    "                \"New messages:\\n{messages}\"\n",
    "            )\n",
    "        ])\n",
    "        # format the messages and invoke the LLM\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=self.messages,\n",
    "                messages=[x.content for x in messages]\n",
    "            )\n",
    "        )\n",
    "        # replace the existing history with a single system summary message\n",
    "        self.messages = [SystemMessage(content=new_summary)]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82f38841",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chat_map = {}\n",
    "\n",
    "def get_summary_chat_history(session_id: str, llm: GoogleGenerativeAI) -> ConversationSummaryMessageHistory:\n",
    "    if session_id not in summary_chat_map:\n",
    "        summary_chat_map[session_id] = ConversationSummaryMessageHistory(llm = llm)\n",
    "    \n",
    "    return summary_chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e74cbbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_csmh = (\n",
    "        { \n",
    "        \"query\": lambda x: x[\"query\"],\n",
    "        \"history\": lambda x: x[\"history\"]\n",
    "        }\n",
    "    | prompt_template\n",
    "    | llm\n",
    ")\n",
    "\n",
    "pipeline_with_summary_history = RunnableWithMessageHistory(\n",
    "    pipeline_csmh,\n",
    "    get_session_history = get_summary_chat_history,\n",
    "    input_messages_key = \"query\",\n",
    "    history_messages_key = \"history\",\n",
    "    history_factory_config = [\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=GoogleGenerativeAI,\n",
    "            name=\"LLM\",\n",
    "            description=\"The LLM to use for the conversation summary\",\n",
    "            default=llm,\n",
    "        )\n",
    "        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a18961c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Muhammad, it's nice to meet you! My name is Zeta. How can I help you today?\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_id = \"csbm_123\"\n",
    "\n",
    "pipeline_with_summary_history.invoke(\n",
    "    {\n",
    "        \"query\": \"Hi, my name is Muhammad\"\n",
    "    },\n",
    "    config={\n",
    "        \"session_id\": session_id, \n",
    "        \"llm\": llm\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15ea49a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='The conversation began with Muhammad introducing himself. Zeta, the AI, responded, introducing herself and offering assistance.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_chat_map[session_id].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc1aee02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, Muhammad. I understand you're researching different types of conversational memory. That's a fascinating area! To help me understand what you're looking for, could you tell me a bit more about what aspects you're interested in? For example, are you focusing on:\\n\\n*   **Specific types of memory architectures?** (e.g., episodic memory, semantic memory, working memory, long-term memory, etc.)\\n*   **Different techniques for implementing conversational memory in AI systems?** (e.g., using embeddings, knowledge graphs, attention mechanisms, etc.)\\n*   **The challenges and limitations of current conversational memory approaches?** (e.g., dealing with context length, maintaining consistency, handling ambiguity, etc.)\\n*   **Applications of conversational memory in specific domains?** (e.g., customer service, education, healthcare, etc.)\\n\\nThe more information you can give me, the better I can assist you in your research. I can provide information, suggest resources, and help you brainstorm ideas. Let me know where you'd like to start!\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_summary_history.invoke(\n",
    "    {\n",
    "        \"query\": \"I'm researching different type of conversational memory\"\n",
    "    },\n",
    "    config={\n",
    "        \"session_id\": session_id, \n",
    "        \"llm\": llm\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59ecdadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in [\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]:\n",
    "    pipeline_with_summary_history.invoke(\n",
    "        {\"query\": msg},\n",
    "        config={\"session_id\": session_id, \"llm\": llm}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b355035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"The conversation began with Muhammad introducing himself to Zeta, the AI, stating he is researching different types of conversational memory. Zeta offered assistance and asked for clarification on the specific aspects of conversational memory Muhammad is interested in, such as memory architectures, implementation techniques, challenges, or applications in specific domains. Muhammad stated he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. Zeta confirmed that ConversationBufferMemory stores the entire conversation history as a single string, making it easy to access the full context but also presenting drawbacks as the conversation grows. Buffer window memory stores the last k messages, dropping the rest, which Zeta confirmed. Zeta then prompted Muhammad to consider the implications and potential issues of storing the entire conversation, and the trade-offs between the two approaches, also inquiring if he is considering other memory types or focusing on these two, and if he's interested in how the choice of memory type impacts performance in a specific application.\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_chat_map[session_id].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c604959a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Muhammad.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    pipeline_with_summary_history.invoke(\n",
    "        {\"query\": \"what is my name again\"},\n",
    "        config={\"session_id\": session_id, \"llm\": llm}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e599850c",
   "metadata": {},
   "source": [
    "### ConversationSummaryBufferMemory\n",
    "\n",
    "Our final memory type acts as a combination of ConversationSummaryMemory and ConversationBufferMemory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3b628f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2375/533772851.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=300,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "csbm1_chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a849f61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[]\n",
      "Human: Hi, my name is James\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Hi, my name is James',\n",
       " 'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hi James! It's nice to meet you. My name is, well, I don't really *have* a name in the traditional sense. You can just call me AI. I'm a large language model, trained by Google. I'm excited to chat with you today! What's on your mind?\", additional_kwargs={}, response_metadata={})],\n",
       " 'response': \"Hi James! It's nice to meet you. My name is, well, I don't really *have* a name in the traditional sense. You can just call me AI. I'm a large language model, trained by Google. I'm excited to chat with you today! What's on your mind?\"}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csbm1_chain.invoke({\"input\": \"Hi, my name is James\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e77e02b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi James! It's nice to meet you. My name is, well, I don't really *have* a name in the traditional sense. You can just call me AI. I'm a large language model, trained by Google. I'm excited to chat with you today! What's on your mind?\", additional_kwargs={}, response_metadata={})]\n",
      "Human: I'm researching the different types of conversational memory.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[SystemMessage(content='The human introduces himself as James. The AI introduces itself as a large language model trained by Google and expresses excitement to chat. James says he is researching the different types of conversational memory, and the AI responds with a detailed explanation of different types of conversational memory, including short-term, long-term, episodic, semantic, working, and contextual memory.', additional_kwargs={}, response_metadata={})]\n",
      "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[SystemMessage(content=\"The human introduces himself as James. The AI introduces itself as a large language model trained by Google and expresses excitement to chat. James says he is researching the different types of conversational memory, and the AI responds with a detailed explanation of different types of conversational memory, including short-term, long-term, episodic, semantic, working, and contextual memory. James specifies he's been looking at ConversationBufferMemory and ConversationBufferWindowMemory, and the AI provides a detailed comparison of the two, outlining their functionality, pros, and cons. ConversationBufferMemory stores the entire conversation history, offering a complete record but consuming more memory and potentially exceeding context window limitations. ConversationBufferWindowMemory, on the other hand, only stores the most recent interactions, making it more memory-efficient and faster but potentially losing older context. The AI summarizes ConversationBufferMemory as a complete transcript and ConversationBufferWindowMemory as short-term memory, and suggests the choice between them depends on the application's needs and trade-offs. The AI then asks if James has specific use cases in mind.\", additional_kwargs={}, response_metadata={})]\n",
      "Human: Buffer memory just stores the entire conversation\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[SystemMessage(content=\"The human introduces himself as James. The AI introduces itself as a large language model trained by Google and expresses excitement to chat. James says he is researching the different types of conversational memory, and the AI responds with a detailed explanation of different types of conversational memory, including short-term, long-term, episodic, semantic, working, and contextual memory. James specifies he's been looking at ConversationBufferMemory and ConversationBufferWindowMemory, and the AI provides a detailed comparison of the two, outlining their functionality, pros, and cons. ConversationBufferMemory stores the entire conversation history, offering a complete record but consuming more memory and potentially exceeding context window limitations. ConversationBufferWindowMemory, on the other hand, only stores the most recent interactions, making it more memory-efficient and faster but potentially losing older context. The AI summarizes ConversationBufferMemory as a complete transcript and ConversationBufferWindowMemory as short-term memory, and suggests the choice between them depends on the application's needs and trade-offs. The AI then asks if James has specific use cases in mind.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Yes, that's correct. `ConversationBufferMemory` essentially acts like a digital recorder for your entire conversation. It diligently appends each new turn of the conversation (both the human's input and the AI's response) to a growing string or list. This means that with each interaction, the memory grows linearly.\\n\\nThink of it like taking notes in a single, ever-expanding document. You start with a blank page, and with each sentence spoken, you add it to the bottom of the page. Eventually, you have a complete record of everything that was said.\\n\\nThis approach has the advantage of providing the AI with complete context, allowing it to potentially draw connections between earlier and later parts of the conversation. However, as we discussed, the downside is the increasing memory consumption and the risk of exceeding the context window limitations of the underlying language model.\\n\\nSo, in short, you're spot on: `ConversationBufferMemory` stores the *entire* conversation history. Is there anything else I can clarify about it?\", additional_kwargs={}, response_metadata={})]\n",
      "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for msg in [\n",
    "    \"I'm researching the different types of conversational memory.\",\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]:\n",
    "    csbm1_chain.invoke({\"input\": msg})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52264f91",
   "metadata": {},
   "source": [
    "### ConversationSummaryBufferMemory with RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "22a85dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummaryBufferMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: GoogleGenerativeAI = Field(default_factory=GoogleGenerativeAI)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, llm: GoogleGenerativeAI, k: int):\n",
    "        super().__init__(llm=llm, k=k)\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages and summarizing the messages that we\n",
    "        drop.\n",
    "        \"\"\"\n",
    "        existing_summary: SystemMessage | None = None\n",
    "        old_messages: list[BaseMessage] | None = None\n",
    "        # see if we already have a summary message\n",
    "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
    "            print(\">> Found existing summary\")\n",
    "            existing_summary = self.messages.pop(0)\n",
    "        # add the new messages to the history\n",
    "        self.messages.extend(messages)\n",
    "        # check if we have too many messages\n",
    "        if len(self.messages) > self.k:\n",
    "            print(\n",
    "                f\">> Found {len(self.messages)} messages, dropping \"\n",
    "                f\"oldest {len(self.messages) - self.k} messages.\"\n",
    "            )\n",
    "            # pull out the oldest messages...\n",
    "            old_messages = self.messages[:self.k]\n",
    "            # ...and keep only the most recent messages\n",
    "            self.messages = self.messages[-self.k:]\n",
    "        if old_messages is None:\n",
    "            print(\">> No old messages to update summary with\")\n",
    "            # if we have no old_messages, we have nothing to update in summary\n",
    "            return\n",
    "        # construct the summary chat messages\n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"Given the existing conversation summary and the new messages, \"\n",
    "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                \"as much relevant information as possible.\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
    "                \"New messages:\\n{old_messages}\"\n",
    "            )\n",
    "        ])\n",
    "        # format the messages and invoke the LLM\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=existing_summary,\n",
    "                old_messages=old_messages\n",
    "            )\n",
    "        )\n",
    "        print(f\">> New summary: {new_summary}\")\n",
    "        # prepend the new summary to the history\n",
    "        self.messages = [SystemMessage(content=new_summary)] + self.messages\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "15a38198",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map_csbm_1 = {}\n",
    "def get_chat_history_csbm_1(session_id: str, llm: GoogleGenerativeAI, k: int) -> ConversationSummaryBufferMessageHistory:\n",
    "    \n",
    "    if session_id not in chat_map_csbm_1:\n",
    "        print(f\"session not exists {session_id}\")\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map_csbm_1[session_id] = ConversationSummaryBufferMessageHistory(llm=llm, k=k)\n",
    "    # return the chat history\n",
    "    return chat_map_csbm_1[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f8dd2659",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_csbmh = (\n",
    "        { \n",
    "        \"query\": lambda x: x[\"query\"],\n",
    "        \"history\": lambda x: x[\"history\"]\n",
    "        }\n",
    "    | prompt_template\n",
    "    | llm\n",
    ")\n",
    "\n",
    "\n",
    "pipeline_with_history_csbm_1 = RunnableWithMessageHistory(\n",
    "    pipeline_csbmh,\n",
    "    get_session_history=get_chat_history_csbm_1,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=GoogleGenerativeAI,\n",
    "            name=\"LLM\",\n",
    "            description=\"The LLM to use for the conversation summary\",\n",
    "            default=llm,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\",\n",
    "            annotation=int,\n",
    "            name=\"k\",\n",
    "            description=\"The number of messages to keep in the history\",\n",
    "            default=4,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "86c74e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session not exists csbm_123\n",
      ">> No old messages to update summary with\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Muhammad, it's nice to meet you! My name is Zeta. How can I help you today?\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csbm_session_id = \"csbm_123\"\n",
    "pipeline_with_history_csbm_1.invoke(\n",
    "        {\"query\": \"Hi, my name is Muhammad\"},\n",
    "        config={\"session_id\": csbm_session_id, \"llm\": llm, \"k\": 4}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "22ea5e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Message 1\n",
      "---\n",
      "\n",
      ">> No old messages to update summary with\n",
      "---\n",
      "Message 2\n",
      "---\n",
      "\n",
      ">> Found 6 messages, dropping oldest 2 messages.\n",
      ">> New summary: Muhammad is researching conversational memory and has introduced himself to Zeta, the AI assistant. Zeta is offering to help and has asked Muhammad to clarify his research interests, providing several examples of possible areas of focus within conversational memory.\n",
      "---\n",
      "Message 3\n",
      "---\n",
      "\n",
      ">> Found existing summary\n",
      ">> Found 6 messages, dropping oldest 2 messages.\n",
      ">> New summary: content='Muhammad is researching conversational memory and has introduced himself to Zeta, the AI assistant. Zeta is helping Muhammad by explaining different types of conversational memory, specifically focusing on ConversationBufferMemory and ConversationBufferWindowMemory. Zeta detailed their functionalities, pros, cons, use cases, and key differences, including how they store information, their scalability, context retention, computational cost, and complexity. Zeta also provided an example and suggested further research avenues.' additional_kwargs={} response_metadata={}\n",
      "---\n",
      "Message 4\n",
      "---\n",
      "\n",
      ">> Found existing summary\n",
      ">> Found 6 messages, dropping oldest 2 messages.\n",
      ">> New summary: content=\"Muhammad is researching conversational memory and has introduced himself to Zeta, the AI assistant. Zeta is helping Muhammad by explaining different types of conversational memory, specifically focusing on ConversationBufferMemory and ConversationBufferWindowMemory. Zeta detailed their functionalities, pros, cons, use cases, and key differences, including how they store information, their scalability, context retention, computational cost, and complexity. Zeta provided an example illustrating how each memory type would store a short conversation. Zeta clarified that ConversationBufferMemory stores the entire conversation history without limitations or summarization and apologized for a previous inaccuracy. Zeta offered to clarify further details about the data structures used or how these memory types interact with language models and suggested further research avenues.\" additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "for i, msg in enumerate([\n",
    "    \"I'm researching the different types of conversational memory.\",\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]):\n",
    "    print(f\"---\\nMessage {i+1}\\n---\\n\")\n",
    "    pipeline_with_history_csbm_1.invoke(\n",
    "        {\"query\": msg},\n",
    "        config={\"session_id\": csbm_session_id, \"llm\": llm, \"k\": 4}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55395090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
